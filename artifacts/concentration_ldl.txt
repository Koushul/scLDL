Directory structure:
└── seutjw-cdl-ld/
    ├── README.md
    ├── data.py
    ├── ldl_metrics.py
    ├── model.py
    └── train.py

================================================
FILE: README.md
================================================
# Code of Concentration Distribution Learning from Label Distributions

## Requirment
- Pytorch 1.3.0
- Python 3
- sklearn
- numpy
- scipy

## Datasets
- Change in line 47 of train.py
- 3 choices:
  - Natural_Scene_split_binary
  - SJA_c_split_binary (the CDL dataset)
  - Yeast_alpha_split_binary

## Outputs
- Mean and Std in [Cheby Clark KL Cosine]


================================================
FILE: data.py
================================================
import numpy as np
import scipy.io as sio
from torch.utils.data import Dataset
from sklearn.preprocessing import MinMaxScaler


class Multi_view_data(Dataset):


    def __init__(self, root, train=True):

        super(Multi_view_data, self).__init__()
        self.root = root
        self.train = train
        data_path = self.root + '.mat'

        dataset = sio.loadmat(data_path)
        view_number = 1
        self.X = dict()
        if train:
            self.X[0] = dataset['X_tr']
            y = dataset['Dr_tr']
        else:
            self.X[0] = dataset['X_test']
            y = dataset['Dr_test']

        self.y = y

    def __getitem__(self, index):
        data = dict()
        for v_num in range(len(self.X)):
            data[v_num] = (self.X[v_num][index]).astype(np.float32)
        target = self.y[index]
        return data, target


    def __len__(self):
        return len(self.X[0])

    def lenx(self):
        return len(self.X[0][0])

    def leny(self):
        return len(self.y[0])


def normalize(x, min=0):
    if min == 0:
        scaler = MinMaxScaler((0, 1))
    else:  # min=-1
        scaler = MinMaxScaler((-1, 1))
    norm_x = scaler.fit_transform(x)
    return norm_x



================================================
FILE: ldl_metrics.py
================================================
import numpy as np
from sklearn.metrics import *
from sklearn.metrics.pairwise import *
from sklearn.preprocessing import normalize

eps = np.finfo(np.float64).eps

#Projecting onto probability simplex
def proj(Y):
    n, m = Y.shape
    X = np.sort(Y, 1)[:, ::-1]
    Xtmp = (np.cumsum(X, 1) - 1) * (1 / (np.arange(m) + 1))
    return np.maximum(Y - np.reshape(Xtmp[np.arange(n), np.sum(X > Xtmp, 1) - 1], (-1, 1)), 0)


def KL_div(Y, Y_hat):
    Y = np.clip(Y, eps, 1)
    Y_hat = np.clip(Y_hat, eps, 1)   
    kl = np.sum(Y * (np.log(Y) - np.log(Y_hat)), 1)
    
    return kl.mean()


def Cheby(Y, Y_hat):
    diff_abs = np.abs(Y - Y_hat)
    cheby = np.max(diff_abs, 1)
    return cheby.mean()


def Clark(Y, Y_hat):
    Y = np.clip(Y, eps, 1)
    Y_hat = np.clip(Y_hat, eps, 1)
    sum_2 = np.power(Y + Y_hat, 2)
    diff_2 = np.power(Y - Y_hat, 2)
    clark = np.sqrt(np.sum(diff_2 / sum_2, 1))
    
    return clark.mean()
    
def Canberra(Y, Y_hat):
    Y = np.clip(Y, eps, 1)
    Y_hat = np.clip(Y_hat, eps, 1)
    
    sum_2 = Y + Y_hat
    diff_abs = np.abs(Y - Y_hat)
    can = np.sum(diff_abs / sum_2, 1)
    
    return can.mean()

def Cosine(Y, Y_hat):
    return 1 - paired_cosine_distances(Y, Y_hat).mean()


def Intersection(Y, Y_hat):
    l1 = np.sum(np.abs(Y - Y_hat), 1)
    return 1 - 0.5 * l1.mean()

def Fidelity(Y, Y_hat):
    sim = np.sqrt(Y * Y_hat)
    fid = np.sum(sim, 1)
    
    return fid.mean()

def Euclidean(Y, Y_hat):
    ecu = paired_euclidean_distances(Y, Y_hat)
    return ecu.mean()
    

def score(Y, Y_hat):

    cheby = Cheby(Y, Y_hat)
    clark = Clark(Y, Y_hat)
    can = Canberra(Y, Y_hat)
    kl = KL_div(Y, Y_hat)
    inter = Intersection(Y, Y_hat)
    
    
    #return (cheby, clark, can, kl, inter)
    cosine = Cosine(Y, Y_hat)
    return (cheby, clark, can, kl, cosine, inter)
    


================================================
FILE: model.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.linalg as LA


# loss function
def KL(alpha, c):
    beta = torch.ones((1, c)).cuda()
    S_alpha = torch.sum(alpha, dim=1, keepdim=True)
    S_beta = torch.sum(beta, dim=1, keepdim=True)
    lnB = torch.lgamma(S_alpha) - torch.sum(torch.lgamma(alpha), dim=1, keepdim=True)
    lnB_uni = torch.sum(torch.lgamma(beta), dim=1, keepdim=True) - torch.lgamma(S_beta)
    dg0 = torch.digamma(S_alpha)
    dg1 = torch.digamma(alpha)
    kl = torch.sum((alpha - beta) * (dg1 - dg0), dim=1, keepdim=True) + lnB + lnB_uni
    return kl



def mse_loss(p, alpha, c, global_step, annealing_step=1):
    S = torch.sum(alpha, dim=1, keepdim=True)
    E = alpha - 1
    m = alpha / S
    label = p
    A = torch.sum((label - m) ** 2, dim=1, keepdim=True)
    B = torch.sum(alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True)
    return A + B


class CDLLD(nn.Module):

    def __init__(self, classes, views, classifier_dims, lambda_epochs=1, lambda_para=0):

        super(CDLLD, self).__init__()
        self.views = views
        self.classes = classes
        self.lambda_epochs = lambda_epochs
        self.lambda_para = lambda_para
        self.Classifiers = nn.ModuleList([Classifier(classifier_dims[i], self.classes) for i in range(self.views)])


    def forward(self, X, y, global_step):
        evidence = self.infer(X)
        loss = 0
        alpha = dict()
        alpha[0] = evidence[0] + 1
        loss += mse_loss(y, alpha[0], self.classes, global_step, self.lambda_epochs)
        + self.lambda_para * sum(p.pow(2.0).sum() for p in self.parameters())
        loss = torch.mean(loss)
        return evidence, loss

    def infer(self, input):
        evidence = dict()
        for v_num in range(self.views):
            evidence[v_num] = self.Classifiers[v_num](input[v_num])
        return evidence


class Classifier(nn.Module):
    def __init__(self, classifier_dims, classes):
        super(Classifier, self).__init__()
        self.num_layers = len(classifier_dims)
        self.fc = nn.ModuleList()
        for i in range(self.num_layers - 1):
            self.fc.append(nn.Linear(classifier_dims[i], classifier_dims[i + 1]))
        self.fc.append(nn.Linear(classifier_dims[self.num_layers - 1], classes))
        self.fc.append(nn.Softplus())
        self.apply(weight_init)

    def forward(self, x):
        h = self.fc[0](x)
        for i in range(1, len(self.fc)):
            h = self.fc[i](h)
        return h

def weight_init(m):
    if isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, mean=0.0, std=1.0)
        nn.init.constant_(m.bias, 0)



================================================
FILE: train.py
================================================
import os
import numpy as np
import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from model import CDLLD
from data import Multi_view_data
import warnings
import ldl_metrics as ldlm

warnings.filterwarnings("ignore")
os.environ["CUDA_VISIBLE_DEVICES"] = "1"


class AverageMeter(object):

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--batch-size', type=int, default=50, metavar='N',
                        help='input batch size for training')
    parser.add_argument('--epochs', type=int, default=20, metavar='N',
                        help='number of epochs to train')
    parser.add_argument('--lambda-epochs', type=int, default=1, metavar='N',
                        help='gradually increase the value of lambda from 0 to 1')
    parser.add_argument('--lr', type=float, default=1e-2, metavar='LR',
                        help='learning rate')
    args = parser.parse_args()
    args.data_name = 'SJA_c_split_binary'
    args.data_path = 'datasets/' + args.data_name

    args.views = 1

    train_loader = torch.utils.data.DataLoader(
        Multi_view_data(args.data_path, train=True), batch_size=args.batch_size, shuffle=True, drop_last=True)
    test_loader = torch.utils.data.DataLoader(
        Multi_view_data(args.data_path, train=False), batch_size=args.batch_size, shuffle=False)
    N_mini_batches = len(train_loader)


    args.dims = [[Multi_view_data(args.data_path, train=False).lenx()]]
    args.classes = classes = Multi_view_data(args.data_path, train=False).leny() - 1


    def train(epoch):
        model.train()
        loss_meter = AverageMeter()
        for batch_idx, (data, target) in enumerate(train_loader):
            for v_num in range(len(data)):
                data[v_num] = Variable(data[v_num].cuda())
            target = Variable(target.cuda())
            # refresh the optimizer
            optimizer.zero_grad()
            evidences, loss = model(data, torch.nn.functional.softmax(target[:, :classes]), epoch)
            # compute gradients and take step
            loss.backward()
            optimizer.step()
            loss_meter.update(loss.item())


    def test(epoch):
        model.eval()
        loss_meter = AverageMeter()
        correct_num, data_num = 0, 0
        Yhat = torch.empty(0, classes).cuda()
        GT = torch.empty(0, classes).cuda()
        for batch_idx, (data, target) in enumerate(test_loader):
            for v_num in range(len(data)):
                data[v_num] = Variable(data[v_num].cuda())
            data_num += target.size(0)
            with torch.no_grad():
                target = Variable(target.cuda())
                evidences, loss = model(data, torch.nn.functional.softmax(target[:, :classes]), epoch)
                #_, predicted = torch.max(evidences.data, 1)
                #print(evidences[0].size())
                if batch_idx == 0:
                    Yhat = evidences[0]
                    GT = target
                else:
                    Yhat = torch.cat((Yhat, evidences[0]), 0)
                    GT = torch.cat((GT, target), 0)
                #print(torch.nn.functional.normalize(target[:, :classes], p=1, dim=1))
                loss_meter.update(loss.item())

        S = torch.sum((Yhat + 1), dim=1, keepdim=True)

        b = Yhat / (S.expand(Yhat.shape))
        u = args.classes / S


        Yhat = torch.cat((b, u), 1)


        GT = GT.cpu().numpy()
        Yhat = Yhat.cpu().numpy()




        metrics = np.array([ldlm.Cheby(GT, Yhat), ldlm.Clark(GT, Yhat),
                            ldlm.KL_div(GT, Yhat), ldlm.Cosine(GT, Yhat)])

        return loss_meter.avg, correct_num / data_num, metrics

    K = 2

    wdd = {'Natural_Scene_split_binary': 1e-5, 'SJA_c_split_binary': 1, 'Yeast_alpha_split_binary': 1e-5}

    for k in range(1, K + 1):

        model = CDLLD(args.classes, args.views, args.dims, args.lambda_epochs)
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=wdd[args.data_name])

        model.cuda()


        for epoch in range(1, args.epochs + 1):
            train(epoch)


        test_loss, acc, metrics = test(args.epochs)
        if k == 1:
            overall_metrics = metrics
        else:
            overall_metrics = np.column_stack((overall_metrics, metrics))

    mean_m = np.mean(overall_metrics, axis=1)
    std_m = np.std(overall_metrics, axis=1)

    print('==================================')
    print('Mean in [Cheby Clark KL Cosine]:')
    print(mean_m)
    print('==================================')
    print('Std in [Cheby Clark KL Cosine]:')
    print(std_m)
    print('==================================')




