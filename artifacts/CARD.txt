Directory structure:
└── xzwhan-card/
    ├── README.md
    ├── environment.yml
    ├── classification/
    │   ├── data_loader.py
    │   ├── diffusion_utils.py
    │   ├── ema.py
    │   ├── main.py
    │   ├── model.py
    │   ├── utils.py
    │   ├── configs/
    │   │   ├── cifar10.yml
    │   │   ├── cifar100.yml
    │   │   ├── fashionmnist.yml
    │   │   ├── gaussmix.yml
    │   │   └── mnist.yml
    │   ├── imagenet/
    │   │   ├── README.md
    │   │   ├── main.py
    │   │   └── card/
    │   │       ├── __init__.py
    │   │       ├── builder.py
    │   │       └── diffusion_utils.py
    │   ├── pretraining/
    │   │   ├── encoder.py
    │   │   └── resnet.py
    │   └── training_scripts/
    │       └── run_cifar10.sh
    └── regression/
        ├── data_loader.py
        ├── diffusion_utils.py
        ├── ema.py
        ├── main.py
        ├── model.py
        ├── utils.py
        ├── configs/
        │   ├── toy_8gauss.yml
        │   ├── toy_full_circle.yml
        │   ├── toy_inverse_sinusoidal_regression_mdn.yml
        │   ├── toy_linear_regression.yml
        │   ├── toy_loglog_cubic_regression.yml
        │   ├── toy_loglog_linear_regression.yml
        │   ├── toy_quadratic_regression.yml
        │   ├── toy_sinusoidal_regression_mdn.yml
        │   ├── uci_boston.yml
        │   ├── uci_concrete.yml
        │   ├── uci_energy.yml
        │   ├── uci_kin8nm.yml
        │   ├── uci_naval.yml
        │   ├── uci_power.yml
        │   ├── uci_protein.yml
        │   ├── uci_wine.yml
        │   ├── uci_yacht.yml
        │   └── uci_year.yml
        └── training_scripts/
            ├── run_toy_8gauss.sh
            ├── run_toy_full_circle.sh
            ├── run_toy_inverse_sinusoidal_regression_mdn.sh
            ├── run_toy_linear_regression.sh
            ├── run_toy_loglog_cubic_regression.sh
            ├── run_toy_loglog_linear_regression.sh
            ├── run_toy_quadratic_regression.sh
            ├── run_toy_sinusoidal_regression_mdn.sh
            ├── run_uci_boston.sh
            ├── run_uci_concrete.sh
            ├── run_uci_energy.sh
            ├── run_uci_kin8nm.sh
            ├── run_uci_naval.sh
            ├── run_uci_power.sh
            ├── run_uci_protein.sh
            ├── run_uci_wine.sh
            ├── run_uci_yacht.sh
            └── run_uci_year.sh

================================================
FILE: README.md
================================================
# CARD: Classification and Regression Diffusion Models

This repo contains the official implementation for the paper [CARD: Classification and Regression Diffusion Models](https://arxiv.org/pdf/2206.07275.pdf) by [Xizewen Han](https://www.linkedin.com/in/xizewenhan/), [Huangjie Zheng](https://huangjiezheng.com/), and [Mingyuan Zhou](https://mingyuanzhou.github.io/). Published in NeurIPS 2022 (poster).

--------------------

## How to Run the Code

### Dependencies

We recommend configuring the environment through [`conda`](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html). Run the following to install and to activate the environment 

```sh
conda env create -f environment.yml
conda activate card
```

The name of the environment is set to **card** by default. You can modify the first line of the `environment.yml` file to set the new environment's name.

### Usage

We organize our code by the type of tasks into the corresponding `regression` and `classification` directories. As we assume the response variable $\boldsymbol{y}$ to reside in the real continuous space for both regression and classification tasks, the code for training and inference of the diffusion models are the same (`diffusion_utils.py` file). The main differences are the handling of the $\boldsymbol{f_{\phi}}$ model (named `self.cond_pred_model` in both `card_regression.py` and `card_classification.py`), the evaluation of trained models (class function `test`), and the neural network architecture (`model.py` file).

We provide the scripts of model training and evaluation for the tasks reported in our paper in the `training_scripts` directory, including:

* Regression
  * 8 toy examples: linear regression, quadratic regression, sinusoidal regression, log-log linear regression, log-log cubic regression, inverse sinusoidal regression, 8 Gaussians, full circle
  * 10 UCI tasks: Boston Housing, Concrete Strength, Energy Efficiency, Kin8nm, Naval Propulsion, Power Plant, Protein Structure, Wine Quality Red, Yacht Hydrodynamics, Year Prediction MSD
* Classification
  * CIFAR-10, CIFAR-100, ImageNet, FashionMNIST, MNIST (noisy)

Note that for the UCI regression tasks, all data and the corresponding split schemes are adapted through [the official repo of MC Dropout](https://github.com/yaringal/DropoutUncertaintyExps), except the [YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd) dataset due to its size. A link to the full UCI dataset that we used for our experiments can be found [here](https://drive.google.com/drive/u/5/folders/16L5Dy9qw3StCY4AvtP98KA5xDZrtcHV3) for download.
  
We provide the following example to run the model on the Boston Housing regression task:

```sh
bash training_scripts/run_uci_boston.sh
```

The configuration for each of the above listed tasks (including data file location, training log and evaluation result directory settings, neural network architecture, optimization hyperparameters, etc.) are provided in the corresponding files in the `configs` directory. For each experimental run, you can find within the following 4 directories:

* `logs`: `stdout.txt` with training logs, `testmetrics.txt` with evaluation metrics
* `tensorboard`: files to track training progress
* `training_image_samples`: plots during training
* `testing_image_samples`: plots when evaluating the model

## Checkpoints

The checkpoints of all 10 UCI regression tasks and all image classification tasks have been uploaded to Google Drive, and can be accessed and downloaded [here](https://drive.google.com/drive/u/5/folders/1hWT3kW7KssDnfBMJVMedFC14JfELmczM).

## References

If you find the code helpful for your research, please consider citing
```bib
@inproceedings{han2022card,
  title={CARD: Classification and Regression Diffusion Models},
  author={Han, Xizewen and Zheng, Huangjie and Zhou, Mingyuan},
  booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
  year={2022}
}
```



================================================
FILE: environment.yml
================================================
name: card
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - _openmp_mutex=4.5=1_gnu
  - blas=1.0=mkl
  - bzip2=1.0.8=h7b6447c_0
  - ca-certificates=2021.10.8=ha878542_0
  - certifi=2021.10.8=py38h578d9bd_1
  - cudatoolkit=11.3.1=h2bc3f7f_2
  - ffmpeg=4.3=hf484d3e_0
  - freetype=2.11.0=h70c0345_0
  - giflib=5.2.1=h7b6447c_0
  - gmp=6.2.1=h2531618_2
  - gnutls=3.6.15=he1e5248_0
  - intel-openmp=2021.4.0=h06a4308_3561
  - jpeg=9d=h7f8727e_0
  - lame=3.100=h7b6447c_0
  - lcms2=2.12=h3be6417_0
  - ld_impl_linux-64=2.35.1=h7274673_9
  - libffi=3.3=he6710b0_2
  - libgcc-ng=9.3.0=h5101ec6_17
  - libgfortran-ng=7.5.0=h14aa051_19
  - libgfortran4=7.5.0=h14aa051_19
  - libgomp=9.3.0=h5101ec6_17
  - libiconv=1.15=h63c8f33_5
  - libidn2=2.3.2=h7f8727e_0
  - libpng=1.6.37=hbc83047_0
  - libstdcxx-ng=9.3.0=hd4cf53a_17
  - libtasn1=4.16.0=h27cfd23_0
  - libtiff=4.2.0=h85742a9_0
  - libunistring=0.9.10=h27cfd23_0
  - libuv=1.40.0=h7b6447c_0
  - libwebp=1.2.0=h89dd481_0
  - libwebp-base=1.2.0=h27cfd23_0
  - lz4-c=1.9.3=h295c915_1
  - mkl=2021.4.0=h06a4308_640
  - mkl-service=2.4.0=py38h7f8727e_0
  - mkl_fft=1.3.1=py38hd3c417c_0
  - mkl_random=1.2.2=py38h51133e4_0
  - ncurses=6.3=h7f8727e_2
  - nettle=3.7.3=hbbd107a_1
  - numpy=1.21.2=py38h20f2e39_0
  - numpy-base=1.21.2=py38h79a1101_0
  - olefile=0.46=pyhd3eb1b0_0
  - openh264=2.1.0=hd408876_0
  - openssl=1.1.1l=h7f8727e_0
  - patsy=0.5.2=pyhd8ed1ab_0
  - pillow=8.4.0=py38h5aabda8_0
  - pip=21.2.4=py38h06a4308_0
  - python=3.8.12=h12debd9_0
  - python-dateutil=2.8.2=pyhd8ed1ab_0
  - python_abi=3.8=2_cp38
  - pytorch=1.10.0=py3.8_cuda11.3_cudnn8.2.0_0
  - pytorch-mutex=1.0=cuda
  - pytz=2021.3=pyhd8ed1ab_0
  - readline=8.1=h27cfd23_0
  - setuptools=58.0.4=py38h06a4308_0
  - six=1.16.0=pyhd3eb1b0_0
  - sqlite=3.36.0=hc218d9a_0
  - statsmodels=0.12.1=py38h5c078b8_2
  - tk=8.6.11=h1ccaba5_0
  - torchaudio=0.10.0=py38_cu113
  - torchvision=0.11.1=py38_cu113
  - typing_extensions=3.10.0.2=pyh06a4308_0
  - wheel=0.37.0=pyhd3eb1b0_1
  - xz=5.2.5=h7b6447c_0
  - zlib=1.2.11=h7b6447c_3
  - zstd=1.4.9=haebb681_0
  - pip:
    - absl-py==1.1.0
    - anyio==3.4.0
    - argon2-cffi==21.1.0
    - astroid==2.12.9
    - attrs==21.2.0
    - babel==2.9.1
    - backcall==0.2.0
    - bleach==4.1.0
    - cachetools==5.2.0
    - cffi==1.15.0
    - charset-normalizer==2.0.7
    - cycler==0.11.0
    - debugpy==1.5.1
    - decorator==5.1.0
    - defusedxml==0.7.1
    - dill==0.3.5.1
    - entrypoints==0.3
    - flake8==5.0.4
    - fonttools==4.28.2
    - google-auth==2.8.0
    - google-auth-oauthlib==0.4.6
    - grpcio==1.47.0
    - idna==3.3
    - importlib-metadata==4.11.4
    - importlib-resources==5.4.0
    - inflect==5.3.0
    - ipykernel==6.5.1
    - ipython==7.29.0
    - ipython-genutils==0.2.0
    - isort==5.10.1
    - jedi==0.18.1
    - jinja2==3.0.3
    - joblib==1.1.0
    - json5==0.9.6
    - jsonschema==4.2.1
    - jupyter-client==7.1.0
    - jupyter-core==4.9.1
    - jupyter-server==1.12.0
    - jupyterlab==3.2.4
    - jupyterlab-pygments==0.1.2
    - jupyterlab-server==2.8.2
    - kiwisolver==1.3.2
    - lazy-object-proxy==1.7.1
    - markdown==3.3.7
    - markupsafe==2.0.1
    - matplotlib==3.5.0
    - matplotlib-inline==0.1.3
    - mccabe==0.7.0
    - mistune==0.8.4
    - nbclassic==0.3.4
    - nbclient==0.5.9
    - nbconvert==6.3.0
    - nbformat==5.1.3
    - nest-asyncio==1.5.1
    - notebook==6.4.6
    - oauthlib==3.2.0
    - packaging==21.3
    - pandas==1.3.0
    - pandocfilters==1.5.0
    - parso==0.8.2
    - pexpect==4.8.0
    - pickleshare==0.7.5
    - platformdirs==2.5.2
    - prometheus-client==0.12.0
    - prompt-toolkit==3.0.22
    - protobuf==3.19.4
    - ptyprocess==0.7.0
    - pyasn1==0.4.8
    - pyasn1-modules==0.2.8
    - pycodestyle==2.9.1
    - pycparser==2.21
    - pydocstyle==6.1.1
    - pyflakes==2.5.0
    - pygments==2.10.0
    - pylama==8.4.1
    - pylint==2.15.2
    - pyparsing==3.0.6
    - pyrsistent==0.18.0
    - pyyaml==6.0
    - pyzmq==22.3.0
    - requests==2.26.0
    - requests-oauthlib==1.3.1
    - rsa==4.8
    - scikit-learn==1.0.1
    - scipy==1.7.2
    - send2trash==1.8.0
    - setuptools-scm==6.3.2
    - sklearn==0.0
    - sniffio==1.2.0
    - snowballstemmer==2.2.0
    - tensorboard==2.9.1
    - tensorboard-data-server==0.6.1
    - tensorboard-plugin-wit==1.8.1
    - terminado==0.12.1
    - testpath==0.5.0
    - threadpoolctl==3.0.0
    - tomli==1.2.2
    - tomlkit==0.11.4
    - torch-tb-profiler==0.4.0
    - tornado==6.1
    - tqdm==4.62.3
    - traitlets==5.1.1
    - urllib3==1.26.7
    - wcwidth==0.2.5
    - webencodings==0.5.1
    - websocket-client==1.2.1
    - werkzeug==2.1.2
    - wrapt==1.14.1
    - zipp==3.6.0



================================================
FILE: classification/data_loader.py
================================================
import torch
import os
import abc
import utils
import numpy as np
import scipy.stats as stats
from torch.utils.data import TensorDataset
from sklearn.preprocessing import StandardScaler


class Gaussians:
    """
    Gaussian mixture distribution sampler.
    noise control the amount of noise injected to make a thicker swiss roll
    """

    def sample(self, n, noise=0.02, mode=2):
        if noise is None:
            noise = 0.02

        if mode == 2:
            scale = 2.
            centers = [
                (1, 0), (0, 1)]
            centers = [(x, y) for x, y in centers]
            temp = []
            labels = []
            for i in range(n):
                point = np.random.randn(2) * .05
                label = np.random.choice(np.arange(len(centers)))
                center = centers[label]
                point[0] += (center[0] + scale) + 5.
                point[1] += (center[1] + scale) + 10.
                temp.append(point)
                labels.append(label)
            temp = np.array(temp)
            labels = np.array(labels)
            temp /= 1.414  # stdev
        else:
            raise NotImplementedError('Toy data is a 2 mode Gaussian mixture for analysis')

        return torch.from_numpy(temp).float(), torch.from_numpy(labels)


class Dataset(object):
    def __init__(self, seed, n_samples):
        self.seed = seed
        self.n_samples = n_samples
        self.eps_samples = None
        utils.set_random_seed(self.seed)

    @abc.abstractmethod
    def create_train_test_dataset(self):
        pass

    def create_noises(self, noise_dict):
        """
        Ref: https://pytorch.org/docs/stable/distributions.html
        :param noise_dict: {"noise_type": "norm", "loc": 0., "scale": 1.}
        """
        print("Create noises using the following parameters:")
        print(noise_dict)
        noise_type = noise_dict.get("noise_type", "norm")
        if noise_type == "t":
            dist = torch.distributions.studentT.StudentT(df=noise_dict.get("df", 10.), loc=noise_dict.get("loc", 0.0),
                                                         scale=noise_dict.get("scale", 1.0))
        elif noise_type == "unif":
            dist = torch.distributions.uniform.Uniform(low=noise_dict.get("low", 0.), high=noise_dict.get("high", 1.))
        elif noise_type == "Chi2":
            dist = torch.distributions.chi2.Chi2(df=noise_dict.get("df", 10.))
        elif noise_type == "Laplace":
            dist = torch.distributions.laplace.Laplace(loc=noise_dict.get("loc", 0.), scale=noise_dict.get("scale", 1.))
        else:  # noise_type == "norm"
            dist = torch.distributions.normal.Normal(loc=noise_dict.get("loc", 0.), scale=noise_dict.get("scale", 1.))

        self.eps_samples = dist.sample((self.n_samples, 1))


class DatasetOneDimensionalX(Dataset):
    def __init__(self, n_samples, seed, label_min_max, normalize_x=False, normalize_y=False):
        super(DatasetOneDimensionalX, self).__init__(seed=seed, n_samples=n_samples)
        self.label_min = label_min_max[0]
        self.label_max = label_min_max[1]
        self.x_samples, self.y, self.y_logits, self.labels = None, None, None, None
        self.dim_x = None
        self.dim_y = None
        self.x_train, self.y_train, self.y_logits_train, self.labels_train = None, None, None, None
        self.x_test, self.y_test, self.y_logits_test, self.labels_test = None, None, None, None
        self.train_n_samples, self.test_n_samples = None, None
        self.train_dataset, self.test_dataset = None, None
        self.normalize_x = normalize_x
        self.normalize_y = normalize_y
        self.scaler_x, self.scaler_y = None, None

    def convert_one_hot_y_to_logit(self):
        """
        Form y prototype in real number by converting one-hot label to logit.
        """
        self.y_logits = torch.logit(torch.nn.functional.normalize(
            torch.clip(self.y, min=self.label_min, max=self.label_max), p=1.0, dim=1))

    def create_train_test_dataset(self, train_ratio=0.8):
        # first create the logit version of y prototypes
        self.convert_one_hot_y_to_logit()
        # split data into train and test set
        utils.set_random_seed(self.seed)
        data_idx = np.arange(self.n_samples)
        np.random.shuffle(data_idx)
        train_size = int(self.n_samples * train_ratio)
        self.x_train, self.y_train, self.y_logits_train, self.labels_train = \
            self.x_samples[data_idx[:train_size]], self.y[data_idx[:train_size]], \
            self.y_logits[data_idx[:train_size]], self.labels[data_idx[:train_size]]
        self.x_test, self.y_test, self.y_logits_test, self.labels_test = \
            self.x_samples[data_idx[train_size:]], self.y[data_idx[train_size:]], \
            self.y_logits[data_idx[train_size:]], self.labels[data_idx[train_size:]]
        self.train_n_samples = self.x_train.shape[0]
        self.test_n_samples = self.x_test.shape[0]
        # standardize x and y if needed
        if self.normalize_x:
            self.normalize_train_test_x()
        if self.normalize_y:
            self.normalize_train_test_y()
        self.train_dataset = TensorDataset(self.x_train, self.y_train, self.y_logits_train, self.labels_train)
        # sort x for easier plotting purpose during test time
        if self.dim_x == 1:
            sorted_idx = torch.argsort(self.x_test, dim=0).squeeze()
            self.x_test = self.x_test[sorted_idx]
            self.y_test = self.y_test[sorted_idx]
            self.y_logits_test = self.y_logits_test[sorted_idx]
            self.labels_test = self.labels_test[sorted_idx]
        self.test_dataset = TensorDataset(self.x_test, self.y_test, self.y_logits_test, self.labels_test)

    def normalize_train_test_x(self):
        self.scaler_x = StandardScaler(with_mean=True, with_std=True)
        self.x_train = torch.from_numpy(
            self.scaler_x.fit_transform(self.x_train).astype(np.float32))
        self.x_test = torch.from_numpy(
            self.scaler_x.transform(self.x_test).astype(np.float32))

    def normalize_train_test_y(self):
        self.scaler_y = StandardScaler(with_mean=True, with_std=True)
        self.y_train = torch.from_numpy(
            self.scaler_y.fit_transform(self.y_train).astype(np.float32))
        self.y_test = torch.from_numpy(
            self.scaler_y.transform(self.y_test).astype(np.float32))


class GaussianMixture(DatasetOneDimensionalX):
    def __init__(self, n_samples, seed, label_min_max, dist_dict, normalize_x=False, normalize_y=False):
        super(GaussianMixture, self).__init__(
            n_samples=n_samples, seed=seed, label_min_max=label_min_max,
            normalize_x=normalize_x, normalize_y=normalize_y)
        self.means, self.sds, self.probs = dist_dict['means'], dist_dict['sds'], dist_dict['probs']
        self.sample_x_and_y(dist_dict)

    def sample_x_and_y(self, dist_dict):
        """
        :param dist_dict: contains Gaussian mixture mean, standard deviation and class probability lists.
        """
        print("Create x and y using the following parameters:")
        print(dist_dict)
        components = [torch.distributions.normal.Normal(
                        loc=self.means[i],
                        scale=self.sds[i]) for i in range(len(self.probs))]
        m = torch.distributions.categorical.Categorical(torch.tensor(self.probs))

        labels = m.sample((self.n_samples, 1))
        self.x_samples = torch.tensor([components[label.item()].sample() for label in labels]).reshape((-1, 1))
        self.y = torch.nn.functional.one_hot(labels).squeeze().float()
        self.dim_x = self.x_samples.shape[1]  # dimension of data input
        self.dim_y = self.y.shape[1]  # dimension of classification output (one-hot label)
        self.labels = labels

    def plot_samples(self):
        fig, axs = plt.subplots(1, 2, figsize=(2 * 8.5, 5), clear=True)
        axs[0].hist(self.x_samples.numpy(), bins=50, density=True)
        x = np.linspace(-3, 4, 1000)
        pdf_all_components = np.array(
            [self.probs[i] * stats.norm.pdf(x, self.means[i], self.sds[i]) for i in range(len(self.probs))]).sum(0)
        axs[0].plot(x, pdf_all_components)
        axs[0].set_title('Sample Histogram and PDF', fontsize=14)
        axs[1].hist([self.x_samples[self.labels == 0].numpy(),
                     self.x_samples[self.labels == 1].numpy(),
                     self.x_samples[self.labels == 2].numpy()],
                    bins=50, density=False, color=['r', 'g', 'b'], label=[0, 1, 2])
        axs[1].legend(loc='best')
        axs[1].set_title('Histogram with Labels', fontsize=14)
        fig.suptitle('Mixture Gaussian Classification', fontsize=16);

    def compute_class_posterior(self, x):
        weighted_pdf_components = [
            self.probs[i] * stats.norm.pdf(x, self.means[i], self.sds[i]) for i in range(len(self.probs))]
        denominator = np.array(weighted_pdf_components).sum(0)
        x_class_posterior = [(class_i_weighted_pdf/denominator).flatten()
                             for class_i_weighted_pdf in weighted_pdf_components]
        return x_class_posterior


class AddGaussianNoise(object):
    """
    Add standard Gaussian noise to MNIST dataset to create noisy MNIST dataset.
    From https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745.
    """
    def __init__(self, mean=0., std=1.):
        self.std = std
        self.mean = mean

    def __call__(self, tensor):
        return tensor + torch.randn(tensor.size()) * self.std + self.mean

    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)


if __name__ == '__main__':
    if not os.path.exists('./data'):
        os.makedirs('./data')

    import matplotlib.pyplot as plt

    plt.style.use('ggplot')

    x, y = Gaussians().sample(10000, mode=2)
    x = x.data.numpy()
    y_vec = torch.nn.functional.one_hot(y).data.numpy()
    fig, axs = plt.subplots(1, 2, figsize=(8, 3))
    axs[0].scatter(x[:, 0], x[:, 1], s=5, c=y);
    axs[0].set_title(r'data distribution')
    axs[1].scatter(y_vec[:, 0], y_vec[:, 1], s=5, c=y);
    axs[1].set_title(r'label distribution')
    plt.tight_layout()
    plt.savefig(os.path.join('data', 'gaussians.pdf'))



================================================
FILE: classification/diffusion_utils.py
================================================
import math
import torch


def make_beta_schedule(schedule="linear", num_timesteps=1000, start=1e-5, end=1e-2):
    if schedule == "linear":
        betas = torch.linspace(start, end, num_timesteps)
    elif schedule == "const":
        betas = end * torch.ones(num_timesteps)
    elif schedule == "quad":
        betas = torch.linspace(start ** 0.5, end ** 0.5, num_timesteps) ** 2
    elif schedule == "jsd":
        betas = 1.0 / torch.linspace(num_timesteps, 1, num_timesteps)
    elif schedule == "sigmoid":
        betas = torch.linspace(-6, 6, num_timesteps)
        betas = torch.sigmoid(betas) * (end - start) + start
    elif schedule == "cosine" or schedule == "cosine_reverse":
        max_beta = 0.999
        cosine_s = 0.008
        betas = torch.tensor(
            [min(1 - (math.cos(((i + 1) / num_timesteps + cosine_s) / (1 + cosine_s) * math.pi / 2) ** 2) / (
                    math.cos((i / num_timesteps + cosine_s) / (1 + cosine_s) * math.pi / 2) ** 2), max_beta) for i in
             range(num_timesteps)])
    elif schedule == "cosine_anneal":
        betas = torch.tensor(
            [start + 0.5 * (end - start) * (1 - math.cos(t / (num_timesteps - 1) * math.pi)) for t in
             range(num_timesteps)])
    return betas


def extract(input, t, x):
    shape = x.shape
    out = torch.gather(input, 0, t.to(input.device))
    reshape = [t.shape[0]] + [1] * (len(shape) - 1)
    return out.reshape(*reshape)


# Forward functions
def q_sample(y, y_0_hat, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, t, noise=None):
    """
    y_0_hat: prediction of pre-trained guidance classifier; can be extended to represent 
        any prior mean setting at timestep T.
    """
    if noise is None:
        noise = torch.randn_like(y).to(y.device)
    sqrt_alpha_bar_t = extract(alphas_bar_sqrt, t, y)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    # q(y_t | y_0, x)
    y_t = sqrt_alpha_bar_t * y + (1 - sqrt_alpha_bar_t) * y_0_hat + sqrt_one_minus_alpha_bar_t * noise
    return y_t


# Reverse function -- sample y_{t-1} given y_t
def p_sample(model, x, y, y_0_hat, y_T_mean, t, alphas, one_minus_alphas_bar_sqrt):
    """
    Reverse diffusion process sampling -- one time step.

    y: sampled y at time step t, y_t.
    y_0_hat: prediction of pre-trained guidance model.
    y_T_mean: mean of prior distribution at timestep T.
    We replace y_0_hat with y_T_mean in the forward process posterior mean computation, emphasizing that 
        guidance model prediction y_0_hat = f_phi(x) is part of the input to eps_theta network, while 
        in paper we also choose to set the prior mean at timestep T y_T_mean = f_phi(x).
    """
    device = next(model.parameters()).device
    z = torch.randn_like(y)
    t = torch.tensor([t]).to(device)
    alpha_t = extract(alphas, t, y)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_one_minus_alpha_bar_t_m_1 = extract(one_minus_alphas_bar_sqrt, t - 1, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    sqrt_alpha_bar_t_m_1 = (1 - sqrt_one_minus_alpha_bar_t_m_1.square()).sqrt()
    # y_t_m_1 posterior mean component coefficients
    gamma_0 = (1 - alpha_t) * sqrt_alpha_bar_t_m_1 / (sqrt_one_minus_alpha_bar_t.square())
    gamma_1 = (sqrt_one_minus_alpha_bar_t_m_1.square()) * (alpha_t.sqrt()) / (sqrt_one_minus_alpha_bar_t.square())
    gamma_2 = 1 + (sqrt_alpha_bar_t - 1) * (alpha_t.sqrt() + sqrt_alpha_bar_t_m_1) / (
        sqrt_one_minus_alpha_bar_t.square())
    eps_theta = model(x, y, t, y_0_hat).to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t)
    # posterior mean
    y_t_m_1_hat = gamma_0 * y_0_reparam + gamma_1 * y + gamma_2 * y_T_mean
    # posterior variance
    beta_t_hat = (sqrt_one_minus_alpha_bar_t_m_1.square()) / (sqrt_one_minus_alpha_bar_t.square()) * (1 - alpha_t)
    y_t_m_1 = y_t_m_1_hat.to(device) + beta_t_hat.sqrt().to(device) * z.to(device)
    return y_t_m_1


# Reverse function -- sample y_0 given y_1
def p_sample_t_1to0(model, x, y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt):
    device = next(model.parameters()).device
    t = torch.tensor([0]).to(device)  # corresponding to timestep 1 (i.e., t=1 in diffusion models)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    eps_theta = model(x, y, t, y_0_hat).to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t)
    y_t_m_1 = y_0_reparam.to(device)
    return y_t_m_1


def y_0_reparam(model, x, y, y_0_hat, y_T_mean, t, one_minus_alphas_bar_sqrt):
    """
    Obtain y_0 reparameterization from q(y_t | y_0), in which noise term is the eps_theta prediction.
    Algorithm 2 Line 4 in paper.
    """
    device = next(model.parameters()).device
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    eps_theta = model(x, y, t, y_0_hat).to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t).to(device)
    return y_0_reparam


def p_sample_loop(model, x, y_0_hat, y_T_mean, n_steps, alphas, one_minus_alphas_bar_sqrt,
                  only_last_sample=False):
    num_t, y_p_seq = None, None
    device = next(model.parameters()).device
    z = torch.randn_like(y_T_mean).to(device)
    cur_y = z + y_T_mean  # sampled y_T
    if only_last_sample:
        num_t = 1
    else:
        y_p_seq = [cur_y]
    for t in reversed(range(1, n_steps)):
        y_t = cur_y
        cur_y = p_sample(model, x, y_t, y_0_hat, y_T_mean, t, alphas, one_minus_alphas_bar_sqrt)  # y_{t-1}
        if only_last_sample:
            num_t += 1
        else:
            y_p_seq.append(cur_y)
    if only_last_sample:
        assert num_t == n_steps
        y_0 = p_sample_t_1to0(model, x, cur_y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt)
        return y_0
    else:
        assert len(y_p_seq) == n_steps
        y_0 = p_sample_t_1to0(model, x, y_p_seq[-1], y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt)
        y_p_seq.append(y_0)
        return y_p_seq



================================================
FILE: classification/ema.py
================================================
import torch.nn as nn

class EMA(object):
    def __init__(self, mu=0.999):
        self.mu = mu
        self.shadow = {}

    def register(self, module):
        for name, param in module.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self, module):
        for name, param in module.named_parameters():
            if param.requires_grad:
                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data

    def ema(self, module):
        for name, param in module.named_parameters():
            if param.requires_grad:
                param.data.copy_(self.shadow[name].data)

    def ema_copy(self, module):
        module_copy = type(module)(module.config).to(module.config.device)
        module_copy.load_state_dict(module.state_dict())
        self.ema(module_copy)
        return module_copy

    def state_dict(self):
        return self.shadow

    def load_state_dict(self, state_dict):
        self.shadow = state_dict



================================================
FILE: classification/main.py
================================================
import argparse
import traceback
import shutil
import logging
import yaml
import sys
import os
import time
import torch
import numpy as np

torch.set_printoptions(sci_mode=False)

parser = argparse.ArgumentParser(description=globals()["__doc__"])

parser.add_argument(
    "--config", type=str, required=True, help="Path to the config file"
)
parser.add_argument('--device', type=int, default=0, help='GPU device id')
parser.add_argument('--thread', type=int, default=4, help='number of threads')
parser.add_argument("--seed", type=int, default=1234, help="Random seed")
parser.add_argument("--test_sample_seed", type=int, default=-1, help="Random seed during test time sampling")
parser.add_argument(
    "--exp", type=str, default="exp", help="Path for saving running related data."
)
parser.add_argument(
    "--doc",
    type=str,
    required=True,
    help="A string for documentation purpose. "
         "Will be the name of the log folder.",
)
parser.add_argument(
    "--dataroot", type=str, default=None,
    help="This argument will overwrite the dataroot in the config if it is not None."
)
parser.add_argument(
    "--comment", type=str, default="", help="A string for experiment comment"
)
parser.add_argument(
    "--verbose",
    type=str,
    default="info",
    help="Verbose level: info | debug | warning | critical",
)
parser.add_argument("--test", action="store_true", help="Whether to test the model")
parser.add_argument("--tune_T", 
                    action="store_true", 
                    help="Whether to tune the scaling temperature parameter for calibration with training set.")
parser.add_argument("--sanity_check", 
                    action="store_true", 
                    help="Whether to quickly check test function implementation by running on only a few subsets.")
parser.add_argument(
    "--sample",
    action="store_true",
    help="Whether to produce samples from the model",
)
parser.add_argument(
    "--train_guidance_only",
    action="store_true",
    help="Whether to only pre-train the guidance classifier f_phi",
)
parser.add_argument(
    "--noise_prior",
    action="store_true",
    help="Whether to apply a noise prior distribution at timestep T",
)
parser.add_argument(
    "--no_cat_f_phi",
    action="store_true",
    help="Whether to not concatenate f_phi as part of eps_theta input",
)
parser.add_argument(
    "--add_ce_loss",
    action="store_true",
    help="Whether to add cross entropy loss",
)
parser.add_argument(
    "--eval_best",
    action="store_true",
    help="Evaluate best model during training, instead of the ckpt stored at the last epoch",
)
parser.add_argument("--fid", action="store_true")
parser.add_argument("--interpolation", action="store_true")
parser.add_argument(
    "--resume_training", action="store_true", help="Whether to resume training"
)
parser.add_argument(
    "-i",
    "--image_folder",
    type=str,
    default="images",
    help="The folder name of samples",
)
parser.add_argument(
    "--n_splits", type=int, default=10, help="total number of runs with different seeds for a specific task"
)
parser.add_argument(
    "--split", type=int, default=0, help="split ID"
)
parser.add_argument(
    "--ni",
    action="store_true",
    help="No interaction. Suitable for Slurm Job launcher",
)
parser.add_argument(
    "--sample_type",
    type=str,
    default="generalized",
    help="sampling approach (generalized or ddpm_noisy)",
)
parser.add_argument(
    "--skip_type",
    type=str,
    default="uniform",
    help="skip according to (uniform or quadratic)",
)
parser.add_argument(
    "--timesteps", type=int, default=None, help="number of steps involved"
)
parser.add_argument(
    "--eta",
    type=float,
    default=0.0,
    help="eta used to control the variances of sigma",
)
parser.add_argument("--sequence", action="store_true")

# loss option
# parser.add_argument(
#     "--simple", action="store_true", default=False, help="Whether use simple loss for L0"
# )
parser.add_argument(
    "--loss", type=str, default='ddpm', help="loss function"
)

parser.add_argument(
    "--num_sample", type=int, default=1, help="number of samples used in forward and reverse"
)

args = parser.parse_args()


def parse_config():
    args.log_path = os.path.join(args.exp, "logs", args.doc)

    # parse config file
    with open(os.path.join(args.config), "r") as f:
        if args.sample or args.test:
            config = yaml.unsafe_load(f)
            new_config = config
        else:
            config = yaml.safe_load(f)
            new_config = dict2namespace(config)

    tb_path = os.path.join(args.exp, "tensorboard", args.doc)
    if not os.path.exists(tb_path):
        os.makedirs(tb_path)
    if not args.ni:
        import torch.utils.tensorboard as tb

    # overwrite if dataroot is not None
    if not args.dataroot is None:
        new_config.data.dataroot = args.dataroot

    if not args.test and not args.sample:
        args.im_path = os.path.join(args.exp, new_config.training.image_folder, args.doc)
        new_config.diffusion.noise_prior = True if args.noise_prior else False
        new_config.model.cat_y_pred = False if args.no_cat_f_phi else True
        if not args.resume_training:
            if not args.timesteps is None:
                new_config.diffusion.timesteps = args.timesteps
            if args.num_sample > 1:
                new_config.diffusion.num_sample = args.num_sample
            if os.path.exists(args.log_path):
                overwrite = False
                if args.ni:
                    overwrite = True
                else:
                    response = input("Folder already exists. Overwrite? (Y/N)")
                    if response.upper() == "Y":
                        overwrite = True

                if overwrite:
                    shutil.rmtree(args.log_path)
                    shutil.rmtree(tb_path)
                    shutil.rmtree(args.im_path)
                    os.makedirs(args.log_path)
                    os.makedirs(args.im_path)
                    if os.path.exists(tb_path):
                        shutil.rmtree(tb_path)
                else:
                    print("Folder exists. Program halted.")
                    sys.exit(0)
            else:
                os.makedirs(args.log_path)
                if not os.path.exists(args.im_path):
                    os.makedirs(args.im_path)

            with open(os.path.join(args.log_path, "config.yml"), "w") as f:
                yaml.dump(new_config, f, default_flow_style=False)

        if not args.ni:
            new_config.tb_logger = tb.SummaryWriter(log_dir=tb_path)
        else:
            new_config.tb_logger = None
        # setup logger
        level = getattr(logging, args.verbose.upper(), None)
        if not isinstance(level, int):
            raise ValueError("level {} not supported".format(args.verbose))

        handler1 = logging.StreamHandler()
        handler2 = logging.FileHandler(os.path.join(args.log_path, "stdout.txt"))
        formatter = logging.Formatter(
            "%(levelname)s - %(filename)s - %(asctime)s - %(message)s"
        )
        handler1.setFormatter(formatter)
        handler2.setFormatter(formatter)
        logger = logging.getLogger()
        logger.addHandler(handler1)
        logger.addHandler(handler2)
        logger.setLevel(level)

    else:
        if args.sample:
            args.im_path = os.path.join(args.exp, new_config.sampling.image_folder, args.doc)
        else:
            args.im_path = os.path.join(args.exp, new_config.testing.image_folder, args.doc)
        level = getattr(logging, args.verbose.upper(), None)
        if not isinstance(level, int):
            raise ValueError("level {} not supported".format(args.verbose))

        handler1 = logging.StreamHandler()
        # saving test metrics to a .txt file
        handler2 = logging.FileHandler(os.path.join(args.log_path, "testmetrics.txt"))
        formatter = logging.Formatter(
            "%(levelname)s - %(filename)s - %(asctime)s - %(message)s"
        )
        handler1.setFormatter(formatter)
        handler2.setFormatter(formatter)
        logger = logging.getLogger()
        logger.addHandler(handler1)
        logger.addHandler(handler2)
        logger.setLevel(level)

        if args.sample or args.test:
            os.makedirs(args.im_path, exist_ok=True)

    # add device
    device_name = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device_name)
    logging.info("Using device: {}".format(device))
    new_config.device = device

    # set random seed
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    torch.backends.cudnn.benchmark = True

    return new_config, logger


def dict2namespace(config):
    namespace = argparse.Namespace()
    for key, value in config.items():
        if isinstance(value, dict):
            new_value = dict2namespace(value)
        else:
            new_value = value
        setattr(namespace, key, new_value)
    return namespace


def main():
    config, logger = parse_config()
    logging.info("Writing log file to {}".format(args.log_path))
    logging.info("Exp instance id = {}".format(os.getpid()))
    logging.info("Exp comment = {}".format(args.comment))

    if args.loss == 'card_onehot_conditional':
        from card_classification import Diffusion
    else:
        raise NotImplementedError("Invalid loss option")

    try:
        runner = Diffusion(args, config, device=config.device)
        start_time = time.time()
        procedure = None
        if args.sample:
            runner.sample()
            procedure = "Sampling"
        elif args.test:
            if config.data.dataset in ['FashionMNIST', 'MNIST', 'CIFAR10', 'CIFAR100', 'IMAGENE100']:
                y_majority_vote_accuracy_all_steps_list = runner.test_image_task()
            else:
                y_majority_vote_accuracy_all_steps_list = runner.test()
            procedure = "Testing"
        else:
            runner.train()
            procedure = "Training"
        end_time = time.time()
        logging.info("\n{} procedure finished. It took {:.4f} minutes.\n\n\n".format(
            procedure, (end_time - start_time) / 60))
        # remove logging handlers
        handlers = logger.handlers[:]
        for handler in handlers:
            logger.removeHandler(handler)
            handler.close()
        # # return test metric lists
        # if args.test:
        #     return y_majority_vote_accuracy_all_steps_list, config
    except Exception:
        logging.error(traceback.format_exc())

    return 0


if __name__ == "__main__":
    args.doc = args.doc + "/split_" + str(args.split)
    if args.test:
        args.config = args.config + args.doc + "/config.yml"
    sys.exit(main())



================================================
FILE: classification/model.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models.resnet import resnet18, resnet50


class ConditionalLinear(nn.Module):
    def __init__(self, num_in, num_out, n_steps):
        super(ConditionalLinear, self).__init__()
        self.num_out = num_out
        self.lin = nn.Linear(num_in, num_out)
        self.embed = nn.Embedding(n_steps, num_out)
        self.embed.weight.data.uniform_()

    def forward(self, x, t):
        out = self.lin(x)
        gamma = self.embed(t)
        out = gamma.view(-1, self.num_out) * out
        return out


class ConditionalModel(nn.Module):
    def __init__(self, config, guidance=False):
        super(ConditionalModel, self).__init__()
        n_steps = config.diffusion.timesteps + 1
        data_dim = config.model.data_dim
        y_dim = config.data.num_classes
        arch = config.model.arch
        feature_dim = config.model.feature_dim
        hidden_dim = config.model.hidden_dim
        self.guidance = guidance
        # encoder for x
        if config.data.dataset == 'toy':
            self.encoder_x = nn.Linear(data_dim, feature_dim)
        elif config.data.dataset in ['FashionMNIST', 'MNIST', 'CIFAR10', 'CIFAR100', 'IMAGENE100']:
            if arch == 'linear':
                self.encoder_x = nn.Sequential(
                    nn.Linear(data_dim, hidden_dim),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Softplus(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.BatchNorm1d(hidden_dim),
                    nn.Softplus(),
                    nn.Linear(hidden_dim, feature_dim)
                )
            elif arch == 'simple':
                self.encoder_x = nn.Sequential(
                    nn.Linear(data_dim, 300),
                    nn.BatchNorm1d(300),
                    nn.ReLU(),
                    nn.Linear(300, 100),
                    nn.BatchNorm1d(100),
                    nn.ReLU(),
                    nn.Linear(100, feature_dim)
                )
            elif arch == 'lenet':
                self.encoder_x = LeNet(feature_dim, config.model.n_input_channels, config.model.n_input_padding)
            elif arch == 'lenet5':
                self.encoder_x = LeNet5(feature_dim, config.model.n_input_channels, config.model.n_input_padding)
            else:
                self.encoder_x = FashionCNN(out_dim=feature_dim)
        else:
            self.encoder_x = ResNetEncoder(arch=arch, feature_dim=feature_dim)
        # batch norm layer
        self.norm = nn.BatchNorm1d(feature_dim)

        # Unet
        if self.guidance:
            self.lin1 = ConditionalLinear(y_dim * 2, feature_dim, n_steps)
        else:
            self.lin1 = ConditionalLinear(y_dim, feature_dim, n_steps)
        self.unetnorm1 = nn.BatchNorm1d(feature_dim)
        self.lin2 = ConditionalLinear(feature_dim, feature_dim, n_steps)
        self.unetnorm2 = nn.BatchNorm1d(feature_dim)
        self.lin3 = ConditionalLinear(feature_dim, feature_dim, n_steps)
        self.unetnorm3 = nn.BatchNorm1d(feature_dim)
        self.lin4 = nn.Linear(feature_dim, y_dim)

    def forward(self, x, y, t, yhat=None):
        x = self.encoder_x(x)
        x = self.norm(x)
        if self.guidance:
            y = torch.cat([y, yhat], dim=-1)
        y = self.lin1(y, t)
        y = self.unetnorm1(y)
        y = F.softplus(y)
        y = x * y
        y = self.lin2(y, t)
        y = self.unetnorm2(y)
        y = F.softplus(y)
        y = self.lin3(y, t)
        y = self.unetnorm3(y)
        y = F.softplus(y)
        return self.lin4(y)


# Simple convnet
# ---------------------------------------------------------------------------------
# Revised from: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
# ---------------------------------------------------------------------------------
class SimNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 5)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)  # flatten all dimensions except batch
        return x


# FashionCNN
# --------------------------------------------------------------------------------------------------
# Revised from: https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy/notebook
# --------------------------------------------------------------------------------------------------
class FashionCNN(nn.Module):

    def __init__(self, out_dim=10, use_for_guidance=False):
        super(FashionCNN, self).__init__()

        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )

        self.use_for_guidance = use_for_guidance
        if self.use_for_guidance:
            self.fc1 = nn.Linear(in_features=64 * 6 * 6, out_features=600)
            self.drop = nn.Dropout2d(0.25)
            self.fc2 = nn.Linear(in_features=600, out_features=120)
            self.fc3 = nn.Linear(in_features=120, out_features=out_dim)
        else:
            self.fc1 = nn.Linear(in_features=64 * 6 * 6, out_features=out_dim)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.view(out.size(0), -1)
        out = self.fc1(out)
        if self.use_for_guidance:
            out = self.drop(out)
            out = self.fc2(out)
            out = self.fc3(out)

        return out


# ResNet 18 or 50 as image encoder
class ResNetEncoder(nn.Module):
    def __init__(self, arch='resnet18', feature_dim=128):
        super(ResNetEncoder, self).__init__()

        self.f = []
        if arch == 'resnet50':
            backbone = resnet50()
        elif arch == 'resnet18':
            backbone = resnet18()
        for name, module in backbone.named_children():
            if not isinstance(module, nn.Linear):
                self.f.append(module)
        # encoder
        self.f = nn.Sequential(*self.f)
        self.featdim = backbone.fc.weight.shape[1]
        self.g = nn.Linear(self.featdim, feature_dim)

    def forward_feature(self, x):
        x = self.f(x)
        feature = torch.flatten(x, start_dim=1)
        feature = self.g(feature)
        return feature

    def forward(self, x):
        feature = self.forward_feature(x)
        return feature


# LeNet
class LeNet(nn.Module):
    def __init__(self, num_classes=10, n_input_channels=1, n_input_padding=2):
        super(LeNet, self).__init__()
        # CIFAR-10 with shape (3, 32, 32): n_input_channels=3, n_input_padding=0
        # FashionMNIST and MNIST with shape (1, 28, 28): n_input_channels=1, n_input_padding=2
        self.conv1 = nn.Conv2d(in_channels=n_input_channels, out_channels=6,
                               kernel_size=5, stride=1, padding=n_input_padding)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,
                               kernel_size=5, stride=1, padding=0)
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120,
                               kernel_size=5, stride=1, padding=0)
        self.linear1 = nn.Linear(120, 84)
        self.linear2 = nn.Linear(84, num_classes)
        self.tanh = nn.Tanh()
        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        x = self.conv1(x)
        x = self.tanh(x)
        x = self.avgpool(x)
        x = self.conv2(x)
        x = self.tanh(x)
        x = self.avgpool(x)
        x = self.conv3(x)
        x = self.tanh(x)
        x = x.reshape(x.shape[0], -1)
        x = self.linear1(x)
        x = self.tanh(x)
        x = self.linear2(x)
        return x


class LeNet5(nn.Module):
    def __init__(self, num_classes=10, n_input_channels=1, n_input_padding=2):
        super(LeNet5, self).__init__()
        # CIFAR-10 with shape (3, 32, 32): n_input_channels=3, n_input_padding=0
        # FashionMNIST and MNIST with shape (1, 28, 28): n_input_channels=1, n_input_padding=2
        self.layer1 = nn.Sequential(
            nn.Conv2d(n_input_channels, 6, kernel_size=5, stride=1, padding=n_input_padding),
            nn.BatchNorm2d(6),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2, stride=2))  # apply average pooling instead
        self.layer2 = nn.Sequential(
            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2, stride=2))
        self.fc0 = nn.Linear(400, 120)
        # self.fc0 = nn.Linear(256, 120)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(120, 84)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(84, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc0(out)
        out = self.relu(out)
        out = self.fc1(out)
        out = self.relu1(out)
        out = self.fc2(out)
        return out



================================================
FILE: classification/utils.py
================================================
import random
import math
import numpy as np
import argparse
import torch
import torch.optim as optim
import torchvision
from torch import nn
from torchvision import transforms


def set_random_seed(seed):
    print(f"\n* Set seed {seed}")
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)  # torch.cuda.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)


def dict2namespace(config):
    namespace = argparse.Namespace()
    for key, value in config.items():
        if isinstance(value, dict):
            new_value = dict2namespace(value)
        else:
            new_value = value
        setattr(namespace, key, new_value)
    return namespace


def sizeof_fmt(num, suffix='B'):
    """
    https://stackoverflow.com/questions/24455615/python-how-to-display-size-of-all-variables
    """
    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:
        if abs(num) < 1024.0:
            return "%3.1f %s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f %s%s" % (num, 'Yi', suffix)


# print("Check memory usage of different variables:")
# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),
#                          key=lambda x: -x[1])[:10]:
#     print("{:>30}: {:>8}".format(name, sizeof_fmt(size)))


def get_optimizer(config_optim, parameters):
    if config_optim.optimizer == 'Adam':
        return optim.Adam(parameters, lr=config_optim.lr, weight_decay=config_optim.weight_decay,
                          betas=(config_optim.beta1, 0.999), amsgrad=config_optim.amsgrad,
                          eps=config_optim.eps)
    elif config_optim.optimizer == 'RMSProp':
        return optim.RMSprop(parameters, lr=config_optim.lr, weight_decay=config_optim.weight_decay)
    elif config_optim.optimizer == 'SGD':
        return optim.SGD(parameters, lr=config_optim.lr, momentum=0.9)
    else:
        raise NotImplementedError(
            'Optimizer {} not understood.'.format(config_optim.optimizer))


def get_optimizer_and_scheduler(config, parameters, epochs, init_epoch):
    scheduler = None
    optimizer = get_optimizer(config, parameters)
    if hasattr(config, "T_0"):
        T_0 = config.T_0
    else:
        T_0 = epochs // (config.n_restarts + 1)
    if config.use_scheduler:
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,
                                                                   T_0=T_0,
                                                                   T_mult=config.T_mult,
                                                                   eta_min=config.eta_min,
                                                                   last_epoch=-1)
        scheduler.last_epoch = init_epoch - 1
    return optimizer, scheduler


def adjust_learning_rate(optimizer, epoch, config):
    """Decay the learning rate with half-cycle cosine after warmup"""
    if epoch < config.training.warmup_epochs:
        lr = config.optim.lr * epoch / config.training.warmup_epochs
    else:
        lr = config.optim.min_lr + (config.optim.lr - config.optim.min_lr) * 0.5 * \
             (1. + math.cos(math.pi * (epoch - config.training.warmup_epochs) / (
                     config.training.n_epochs - config.training.warmup_epochs)))
    for param_group in optimizer.param_groups:
        if "lr_scale" in param_group:
            param_group["lr"] = lr * param_group["lr_scale"]
        else:
            param_group["lr"] = lr
    return lr


def get_dataset(args, config):
    data_object = None
    if config.data.dataset == 'toy':
        tr_x, tr_y = Gaussians().sample(config.data.dataset_size)
        te_x, te_y = Gaussians().sample(config.data.dataset_size)
        train_dataset = torch.utils.data.TensorDataset(tr_x, tr_y)
        test_dataset = torch.utils.data.TensorDataset(te_x, te_y)
    elif config.data.dataset == 'MNIST':
        if config.data.noisy:
            # noisy MNIST as in Contextual Dropout --  no normalization, add standard Gaussian noise
            transform = transforms.Compose([
                transforms.ToTensor(),
                # transforms.Normalize((0.1307,), (0.3081,)),
                AddGaussianNoise(0., 1.)
            ])
        else:
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])
        train_dataset = torchvision.datasets.MNIST(root=config.data.dataroot, train=True, download=True,
                                                   transform=transform)
        test_dataset = torchvision.datasets.MNIST(root=config.data.dataroot, train=False, download=True,
                                                  transform=transform)
    elif config.data.dataset == 'FashionMNIST':
        transform = transforms.Compose([transforms.ToTensor(),
                                        transforms.Normalize((0.5,), (0.5,))])
        train_dataset = torchvision.datasets.FashionMNIST(root=config.data.dataroot, train=True, download=True,
                                                          transform=transform)
        test_dataset = torchvision.datasets.FashionMNIST(root=config.data.dataroot, train=False, download=True,
                                                         transform=transform)
    elif config.data.dataset == "CIFAR10":
        data_norm_mean, data_norm_std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
        # data_norm_mean, data_norm_std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)
        transform = transforms.Compose(
            [transforms.ToTensor(),
             transforms.Normalize(mean=data_norm_mean, std=data_norm_std)
             ])
        train_dataset = torchvision.datasets.CIFAR10(root=config.data.dataroot, train=True,
                                                     download=True, transform=transform)
        test_dataset = torchvision.datasets.CIFAR10(root=config.data.dataroot, train=False,
                                                    download=True, transform=transform)
    elif config.data.dataset == "CIFAR100":
        data_norm_mean, data_norm_std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
        # data_norm_mean, data_norm_std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)
        transform = transforms.Compose(
            [transforms.ToTensor(),
             transforms.Normalize(mean=data_norm_mean, std=data_norm_std)
             ])
        train_dataset = torchvision.datasets.CIFAR100(root=config.data.dataroot, train=True,
                                                      download=True, transform=transform)
        test_dataset = torchvision.datasets.CIFAR100(root=config.data.dataroot, train=False,
                                                     download=True, transform=transform)
    elif config.data.dataset == "gaussian_mixture":
        data_object = GaussianMixture(n_samples=config.data.dataset_size,
                                      seed=args.seed,
                                      label_min_max=config.data.label_min_max,
                                      dist_dict=vars(config.data.dist_dict),
                                      normalize_x=config.data.normalize_x,
                                      normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        train_dataset, test_dataset = data_object.train_dataset, data_object.test_dataset
    else:
        raise NotImplementedError(
            "Options: toy (classification of two Gaussian), MNIST, FashionMNIST, CIFAR10.")
    return data_object, train_dataset, test_dataset


# ------------------------------------------------------------------------------------
# Revised from timm == 0.3.2
# https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/metrics.py
# output: the prediction from diffusion model (B x n_classes)
# target: label indices (B)
# ------------------------------------------------------------------------------------
def accuracy(output, target, topk=(1,)):
    """
    Computes the accuracy over the k top predictions for the specified values of k.
    """
    maxk = min(max(topk), output.size()[1])
    # output = torch.softmax(-(output - 1)**2,  dim=-1)
    batch_size = target.size(0)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.reshape(1, -1).expand_as(pred))
    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]


def cast_label_to_one_hot_and_prototype(y_labels_batch, config, return_prototype=True):
    """
    y_labels_batch: a vector of length batch_size.
    """
    y_one_hot_batch = nn.functional.one_hot(y_labels_batch, num_classes=config.data.num_classes).float()
    if return_prototype:
        label_min, label_max = config.data.label_min_max
        y_logits_batch = torch.logit(nn.functional.normalize(
            torch.clip(y_one_hot_batch, min=label_min, max=label_max), p=1.0, dim=1))
        return y_one_hot_batch, y_logits_batch
    else:
        return y_one_hot_batch



================================================
FILE: classification/configs/cifar10.yml
================================================
data:
    dataset: "CIFAR10"
    seed: 2000
    label_min_max: [0.001, 0.999]
    num_classes: 10
    num_workers: 4
    dataroot: '/data/datasets/images/cifar10'

model:
    type: "simple"
    data_dim: 3072
#    data_dim: [3, 32, 32]
    n_input_channels: 3
    n_input_padding: 0
    feature_dim: 4096
    hidden_dim: 4096
    cat_x: True
    cat_y_pred: True
    arch: linear
#    arch: resnet18
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal, cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    include_guidance: True  # concat y_t with aux pred as eps_theta input
    apply_aux_cls: True
    trained_aux_cls_ckpt_path: "/data/classification/pretrained/cifar10_ckpt"
    trained_aux_cls_ckpt_name: "snapshot_supervised.ckpt"  # test accuracy 90.39%, applied in the paper
#    trained_aux_cls_ckpt_name: "snapshot_self_supervised.ckpt"  # test accuracy 94.31%
    aux_cls:
        arch: resnet18_ckpt  # lenet5
        pre_train: False
        joint_train: False
        n_pretrain_epochs: 10
        logging_interval: 1
    
training:
    batch_size: 256
    n_epochs: 1000
    warmup_epochs: 40
    add_t0_loss: False
    n_steps_req_grad: 100
    n_minibatches_add_ce: 20
    n_ce_epochs_warmup: 10
    n_ce_epochs_interval: 50
    n_sanity_check_epochs_freq: 500
    snapshot_freq: 1000000000
    logging_freq: 1200
    validation_freq: 10
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 250
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_samples: 100
    n_bins: 10
    compute_metric_all_steps: False
    metrics_t: 0
    ttest_alpha: 0.05
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: True
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: False
    eps: 0.00000001
    grad_clip: 1.0
    lr_schedule: True
    min_lr: 0.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: classification/configs/cifar100.yml
================================================
data:
    dataset: "CIFAR100"
    seed: 2000
    label_min_max: [0.001, 0.999]
    num_classes: 100
    num_workers: 4
    dataroot: '/data/datasets/images/cifar100'

model:
    type: "simple"
    data_dim: 3072
#    data_dim: [3, 32, 32]
    n_input_channels: 3
    n_input_padding: 0
    feature_dim: 6144
    hidden_dim: 6144
    cat_x: True
    cat_y_pred: True
    arch: linear
#    arch: resnet18
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal, cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    include_guidance: True  # concat y_t with aux pred as eps_theta input
    apply_aux_cls: True
    trained_aux_cls_ckpt_path: "/data/classification/pretrained/cifar100_ckpt"
    trained_aux_cls_ckpt_name: "snapshot_supervised.ckpt"
#    trained_aux_cls_ckpt_name: "snapshot_self_supervised.ckpt"
    aux_cls:
        arch: resnet18_ckpt  # lenet5
        pre_train: False
        joint_train: False
        n_pretrain_epochs: 10
        logging_interval: 1
    
training:
    batch_size: 256
    n_epochs: 1000
    warmup_epochs: 40
    add_t0_loss: False
    n_steps_req_grad: 100
    n_minibatches_add_ce: 20
    n_ce_epochs_warmup: 10
    n_ce_epochs_interval: 50
    n_sanity_check_epochs_freq: 500
    snapshot_freq: 1000000000
    logging_freq: 1200
    validation_freq: 10
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 250
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_samples: 100
    n_bins: 10
    compute_metric_all_steps: False
    metrics_t: 0
    ttest_alpha: 0.05
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: True
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: False
    eps: 0.00000001
    grad_clip: 1.0
    lr_schedule: True
    min_lr: 0.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: classification/configs/fashionmnist.yml
================================================
data:
    dataset: "FashionMNIST"
    seed: 2000
    label_min_max: [0.001, 0.999]
    num_classes: 10
    num_workers: 4
    dataroot: '/data/datasets/images/FashionMNIST'
    
model:
    type: "simple"
    data_dim: 784
    n_input_channels: 1
    n_input_padding: 2
    feature_dim: 2048
    hidden_dim: 2048
    cat_x: True
    cat_y_pred: True
    arch: lenet5
    # arch: linear
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal, cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    include_guidance: True  # concat y_t with aux pred as eps_theta input
    apply_aux_cls: True
#    trained_aux_cls_log_path:
    aux_cls:
        arch: lenet5
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 1
    
training:
    batch_size: 256
    n_epochs: 5000
    warmup_epochs: 100
    add_t0_loss: False
    lambda_ce: 0.01
    n_steps_req_grad: 100
    n_minibatches_add_ce: 20
    n_ce_epochs_warmup: 10
    n_ce_epochs_interval: 50
    n_sanity_check_epochs_freq: 500
    snapshot_freq: 1000000000
    logging_freq: 2400
    validation_freq: 100
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 250
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_samples: 100
    n_bins: 10
    compute_metric_all_steps: False
    metrics_t: 0
    ttest_alpha: 0.01
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: True
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0
    lr_schedule: True
    min_lr: 0.0


aux_optim:
    weight_decay: 0.0001
    optimizer: "Adam"
    lr: 0.0001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: classification/configs/gaussmix.yml
================================================
data:
    dataset: "gaussian_mixture"
    dataset_size: 10240
    seed: 2000
    dist_dict: {"means": [-1, 0.5, 2], "sds": [0.4, 0.45, 0.3], "probs": [0.5, 0.2, 0.3]}
    label_min_max: [0.001, 0.999]
    num_classes: 3
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 1
    feature_dim: 128
    cat_x: True
    cat_y_pred: True
    arch: linear
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10

training:
    batch_size: 256
    n_epochs: 5000
    add_t0_loss: False
    n_ce_epochs_warmup: 10
    n_ce_epochs_interval: 100
    n_sanity_check_epochs_freq: 500
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 320
    validation_freq: 16000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_samples: 100
    n_bins: 10
    compute_metric_all_steps: True
    metrics_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0
    lr_schedule: false

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0
    lr_schedule: false



================================================
FILE: classification/configs/mnist.yml
================================================
data:
    dataset: "MNIST"
    noisy: True
    seed: 2000
    label_min_max: [0.001, 0.999]
    num_classes: 10
    num_workers: 4
    dataroot: '/data/datasets/images/MNIST'

model:
    type: "simple"
    data_dim: 784
    n_input_channels: 1
    n_input_padding: 2
    feature_dim: 128
    hidden_dim: 128
    cat_x: True
    cat_y_pred: True
    arch: linear
    # arch: simple
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal, cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    include_guidance: True  # concat y_t with aux pred as eps_theta input
    apply_aux_cls: True
    aux_cls:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 50
        logging_interval: 5

training:
    batch_size: 128
    n_epochs: 5000
    warmup_epochs: 50
    add_t0_loss: False
    lambda_ce: 0.01
    n_steps_req_grad: 100
    n_minibatches_add_ce: 20
    n_ce_epochs_warmup: 10
    n_ce_epochs_interval: 50
    n_sanity_check_epochs_freq: 500
    snapshot_freq: 1000000000
    logging_freq: 2400
    validation_freq: 100
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 250
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_samples: 100
    n_bins: 10
    compute_metric_all_steps: False
    metrics_t: 0
    ttest_alpha: 0.05
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: True
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0
    lr_schedule: True
    min_lr: 0.0


aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: classification/imagenet/README.md
================================================
# CARD: Classification and Regression Diffusion Models - ImageNet experiments

This repo contains the official implementation for the paper [CARD: Classification and Regression Diffusion Models](https://arxiv.org/pdf/2206.07275.pdf) by [Xizewen Han](https://www.linkedin.com/in/xizewenhan/), [Huangjie Zheng](https://huangjiezheng.com/), and [Mingyuan Zhou](https://mingyuanzhou.github.io/). Published in NeurIPS 2022 (poster). This folder provide instruction for running large-scale experiments, e.g., ImageNet experiments with CARD.

--------------------

## How to Run the Code

### Usage

**Preparation**
Before starting running the code, please first download the checkpoint weight of $\boldsymbol{f_{\phi}}$ (ResNet50 here) from [here](https://download.pytorch.org/models/resnet50-11ad3fa6.pth).

We use standard ImageNet dataset, you can download it from http://image-net.org/. We provide the following two ways to
load data:

- For standard folder dataset, move validation images to labeled sub-folders. The file structure should look like:
  ```bash
  $ tree data
  imagenet
  ├── train
  │   ├── class1
  │   │   ├── img1.jpeg
  │   │   ├── img2.jpeg
  │   │   └── ...
  │   ├── class2
  │   │   ├── img3.jpeg
  │   │   └── ...
  │   └── ...
  └── val
      ├── class1
      │   ├── img4.jpeg
      │   ├── img5.jpeg
      │   └── ...
      ├── class2
      │   ├── img6.jpeg
      │   └── ...
      └── ...
 
  ```

**Running code**

```python
python main.py \
        -a resnet50 --epochs 200 \
        --multiprocessing-distributed --dist-url tcp://$MASTER_ADDR:$MASTER_PORT \
        --world-size $WORLD_SIZE --rank $RANK \
        --pretrained resnet50-11ad3fa6.pth \
        --num_classes 1000 --lars \
        /path/to/imagenet 
```
This command enables training with multiple GPUs and multiple nodes. For single node (single/multi-GPU), please set MASTER_ADDR=localhost, MASTER_PORT as any preferred number, WORLD_SIZE=1 and RANK=0. For multi-node training, please refer to your system configuration, and set these os parameters accordingly.


### Acknowledgement

This repo is built and modified from the repo of [MoCo](https://github.com/facebookresearch/moco) and [Simsiam](https://github.com/facebookresearch/simsiam). We appreciate their great work.

## References

If you find the code helpful for your research, please consider citing
```bib
@inproceedings{han2022card,
  title={CARD: Classification and Regression Diffusion Models},
  author={Han, Xizewen and Zheng, Huangjie and Zhou, Mingyuan},
  booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
  year={2022}
}
```



================================================
FILE: classification/imagenet/main.py
================================================
#!/usr/bin/env python
# Built by Huangjie Zheng
# Modified from Linear probe file in SimSiam: https://github.com/facebookresearch/simsiam/blob/main/main_lincls.py 
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import argparse
import builtins
import math
import os
import random
import shutil
import time
import warnings
from pathlib import Path

import card.builder
import card.diffusion_utils
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.optim
import torch.multiprocessing as mp
import torch.utils.data
import torch.utils.data.distributed
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models

model_names = sorted(name for name in models.__dict__
                     if name.islower() and not name.startswith("__")
                     and callable(models.__dict__[name]))

parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
parser.add_argument('data', metavar='DIR',
                    help='path to dataset')
parser.add_argument('-a', '--arch', metavar='ARCH', default='resnet50',
                    choices=model_names,
                    help='model architecture: ' +
                         ' | '.join(model_names) +
                         ' (default: resnet50)')
parser.add_argument('-j', '--workers', default=32, type=int, metavar='N',
                    help='number of data loading workers (default: 32)')
parser.add_argument('--epochs', default=90, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
                    help='manual epoch number (useful on restarts)')
parser.add_argument('-b', '--batch-size', default=4096, type=int,
                    metavar='N',
                    help='mini-batch size (default: 4096), this is the total '
                         'batch size of all GPUs on the current node when '
                         'using Data Parallel or Distributed Data Parallel')
parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,
                    metavar='LR', help='initial (base) learning rate', dest='lr')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--wd', '--weight-decay', default=0., type=float,
                    metavar='W', help='weight decay (default: 0.)',
                    dest='weight_decay')
parser.add_argument('-p', '--print-freq', default=10, type=int,
                    metavar='N', help='print frequency (default: 10)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
                    help='evaluate model on validation set')
parser.add_argument('--world-size', default=-1, type=int,
                    help='number of nodes for distributed training')
parser.add_argument('--rank', default=-1, type=int,
                    help='node rank for distributed training')
parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,
                    help='url used to set up distributed training')
parser.add_argument('--dist-backend', default='nccl', type=str,
                    help='distributed backend')
parser.add_argument('--seed', default=None, type=int,
                    help='seed for initializing training.')
parser.add_argument('--num_classes', default=100, type=int,
                    help='number of classes of the linear head.')
parser.add_argument('--num_timesteps', default=10, type=int,
                    help='number of diffusion steps.')
parser.add_argument('--beta_1', default=0.01, type=float,
                    help='beta_1 of the linear variance schedule for the diffusion process.')
parser.add_argument('--beta_T', default=0.95, type=float,
                    help='beta_T of the linear variance schedule for the diffusion process.')
parser.add_argument('--gpu', default=None, type=int,
                    help='GPU id to use.')
parser.add_argument('--multiprocessing-distributed', action='store_true',
                    help='Use multi-processing distributed training to launch '
                         'N processes per node, which has N GPUs. This is the '
                         'fastest way to use PyTorch for either single node or '
                         'multi node data parallel training')
parser.add_argument('--output_dir', default=".", type=str, help='Path to save logs and checkpoints.')

# additional configs:
parser.add_argument('--pretrained', default='', type=str,
                    help='path to pretrained checkpoint')
parser.add_argument('--lars', action='store_true',
                    help='Use LARS')

best_acc1 = 0


def main(args):
    if args.seed is not None:
        random.seed(args.seed)
        torch.manual_seed(args.seed)
        cudnn.deterministic = True
        warnings.warn('You have chosen to seed training. '
                      'This will turn on the CUDNN deterministic setting, '
                      'which can slow down your training considerably! '
                      'You may see unexpected behavior when restarting '
                      'from checkpoints.')

    if args.gpu is not None:
        warnings.warn('You have chosen a specific GPU. This will completely '
                      'disable data parallelism.')

    if args.dist_url == "env://" and args.world_size == -1:
        args.world_size = int(os.environ["WORLD_SIZE"])

    args.distributed = args.world_size > 1 or args.multiprocessing_distributed

    ngpus_per_node = torch.cuda.device_count()
    if args.multiprocessing_distributed:
        # Since we have ngpus_per_node processes per node, the total world_size
        # needs to be adjusted accordingly
        args.world_size = ngpus_per_node * args.world_size
        # Use torch.multiprocessing.spawn to launch distributed processes: the
        # main_worker process function
        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))
    else:
        # Simply call main_worker function
        main_worker(args.gpu, ngpus_per_node, args)


def main_worker(gpu, ngpus_per_node, args):
    global best_acc1
    args.gpu = gpu

    # suppress printing if not master
    if args.multiprocessing_distributed and args.gpu != 0:
        def print_pass(*args):
            pass

        builtins.print = print_pass

    if args.gpu is not None:
        print("Use GPU: {} for training".format(args.gpu))

    if args.distributed:
        if args.dist_url == "env://" and args.rank == -1:
            args.rank = int(os.environ["RANK"])
        if args.multiprocessing_distributed:
            # For multiprocessing distributed training, rank needs to be the
            # global rank among all the processes
            args.rank = args.rank * ngpus_per_node + gpu
        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                world_size=args.world_size, rank=args.rank)
        torch.distributed.barrier()
    # create model
    print("=> creating CARD model with backbone: '{}'".format(args.arch))
    model = card.builder.CARD(
        models.__dict__[args.arch],
        args.num_classes, args.num_timesteps)

    print(model)
    print("=> CARD model parameter size: '{}'".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))

    # freeze all layers but the unet
    for name, param in model.named_parameters():
        param.requires_grad = False
    for name, param in model.model_theta.named_parameters():
        param.requires_grad = True

    # load from pre-trained, before DistributedDataParallel constructor
    if args.pretrained:
        if os.path.isfile(args.pretrained):
            print("=> loading checkpoint '{}'".format(args.pretrained))
            checkpoint = torch.load(args.pretrained, map_location="cpu")

            # rename pre-trained keys
            state_dict = checkpoint['state_dict']
            state_dict_encoder = {}
            state_dict_linear = {}
            for k in list(state_dict.keys()):
                # retain only encoder up to before the embedding layer
                if not k.startswith('module.fc'):
                    # remove prefix
                    state_dict_encoder[k[len("module."):]] = state_dict[k]
                else:
                    state_dict_linear[k[len("module.fc."):]] = state_dict[k]

            args.start_epoch = 0
            msg = model.model_phi.load_state_dict(state_dict_encoder)
            print(msg)
            msg = model.linear.load_state_dict(state_dict_linear)
            print(msg)

            print("=> loaded pre-trained model '{}'".format(args.pretrained))
        else:
            print("=> no checkpoint found at '{}'".format(args.pretrained))

    # infer learning rate before changing batch size
    init_lr = args.lr * args.batch_size / 256

    if args.distributed:
        # For multiprocessing distributed, DistributedDataParallel constructor
        # should always set the single device scope, otherwise,
        # DistributedDataParallel will use all available devices.
        if args.gpu is not None:
            torch.cuda.set_device(args.gpu)
            model.cuda(args.gpu)
            # When using a single GPU per process and per
            # DistributedDataParallel, we need to divide the batch size
            # ourselves based on the total number of GPUs we have
            args.batch_size = int(args.batch_size / ngpus_per_node)
            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)
            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)
        else:
            model.cuda()
            # DistributedDataParallel will divide and allocate batch_size to all
            # available GPUs if device_ids are not set
            model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)
    elif args.gpu is not None:
        torch.cuda.set_device(args.gpu)
        model = model.cuda(args.gpu)
    else:
        # DataParallel will divide and allocate batch_size to all available GPUs
        if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):
            model.features = torch.nn.DataParallel(model.features)
            model.cuda()
        else:
            model = torch.nn.DataParallel(model).cuda()

    # define loss function (criterion) and optimizer
    criterion = nn.MSELoss().cuda(args.gpu)

    # optimize only the linear classifier
    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))

    optimizer = torch.optim.SGD(parameters, init_lr,
                                momentum=args.momentum,
                                weight_decay=args.weight_decay)
    if args.lars:
        print("=> use LARS optimizer.")
        from apex.parallel.LARC import LARC
        optimizer = LARC(optimizer=optimizer, trust_coefficient=.001, clip=False)

    # optionally resume from a checkpoint
    if args.resume:
        if os.path.isfile(args.resume):
            print("=> loading checkpoint '{}'".format(args.resume))
            if args.gpu is None:
                checkpoint = torch.load(args.resume)
            else:
                # Map model to be loaded to specified single gpu.
                loc = 'cuda:{}'.format(args.gpu)
                checkpoint = torch.load(args.resume, map_location=loc)
            args.start_epoch = checkpoint['epoch']
            best_acc1 = checkpoint['best_acc1']
            if args.gpu is not None:
                # best_acc1 may be from a checkpoint from a different GPU
                best_acc1 = best_acc1.to(args.gpu)
            model.load_state_dict(checkpoint['state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer'])
            print("=> loaded checkpoint '{}' (epoch {})"
                  .format(args.resume, checkpoint['epoch']))
        else:
            print("=> no checkpoint found at '{}'".format(args.resume))

    cudnn.benchmark = True

    # Data loading code
    traindir = os.path.join(args.data, 'train')
    valdir = os.path.join(args.data, 'val')
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

    train_dataset = datasets.ImageFolder(
        traindir,
        transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ]))

    if args.distributed:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    else:
        train_sampler = None

    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
        num_workers=args.workers, pin_memory=True, sampler=train_sampler)

    val_loader = torch.utils.data.DataLoader(
        datasets.ImageFolder(valdir, transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            normalize,
        ])),
        batch_size=256, shuffle=False,
        num_workers=args.workers, pin_memory=True)

    if args.evaluate:
        validate(val_loader, model, criterion, args)
        return

    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            train_sampler.set_epoch(epoch)
        adjust_learning_rate(optimizer, init_lr, epoch, args)

        # train for one epoch
        train(train_loader, model, criterion, optimizer, epoch, args)

        # evaluate on validation set
        acc1 = validate(val_loader, model, criterion, args)

        # remember best acc@1 and save checkpoint
        is_best = acc1 > best_acc1
        best_acc1 = max(acc1, best_acc1)

        if not args.multiprocessing_distributed or (args.multiprocessing_distributed
                                                    and args.rank % ngpus_per_node == 0):
            save_checkpoint({
                'epoch': epoch + 1,
                'arch': args.arch,
                'state_dict': model.state_dict(),
                'best_acc1': best_acc1,
                'optimizer': optimizer.state_dict(),
            }, is_best, os.path.join(args.output_dir, f'checkpoint{(epoch + 1):04}.pth'))
            if epoch == args.start_epoch:
                sanity_check(model.module.model_phi.state_dict(), args.pretrained)

    if not args.multiprocessing_distributed or (args.multiprocessing_distributed and args.rank % ngpus_per_node == 0):
        with open(os.path.join(args.output_dir, f'best_acc.txt'), 'w') as best_acc_result:
            best_acc_result.write("Best acc: " + str(best_acc1))
        save_prediction(val_loader, model, criterion, args)


def train(train_loader, model, criterion, optimizer, epoch, args):
    batch_time = AverageMeter('Time', ':6.3f')
    data_time = AverageMeter('Data', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    progress = ProgressMeter(
        len(train_loader),
        [batch_time, data_time, losses],
        prefix="Epoch: [{}]".format(epoch))

    model.module.model_theta.train()
    """
    Switch to eval mode:
    It is not legitimate to change any part of the pre-trained model.
    BatchNorm in train mode may revise running mean/std (even if it receives
    no gradient), which are part of the model parameters too.
    """
    model.module.model_phi.eval()

    # diffusion configs
    betas = card.diffusion_utils.make_beta_schedule(schedule="linear", num_timesteps=args.num_timesteps, 
        start=args.beta_1, end=args.beta_T).cuda(
        args.gpu, non_blocking=True)
    betas_sqrt = torch.sqrt(betas)
    alphas = 1.0 - betas
    one_minus_betas_sqrt = torch.sqrt(alphas)
    alphas_cumprod = alphas.cumprod(dim=0)
    alphas_bar_sqrt = torch.sqrt(alphas_cumprod)
    one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_cumprod)
    alphas_cumprod_prev = torch.cat(
        [torch.ones(1).cuda(args.gpu, non_blocking=True), alphas_cumprod[:-1]], dim=0
    )
    posterior_mean_coeff_1 = (
            betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)
    )
    posterior_mean_coeff_2 = (
            torch.sqrt(alphas) * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)
    )
    posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
    )

    end = time.time()
    for i, (images, target) in enumerate(train_loader):
        # measure data loading time
        data_time.update(time.time() - end)

        if args.gpu is not None:
            images = images.cuda(args.gpu, non_blocking=True)
        target = nn.functional.one_hot(target, num_classes=args.num_classes).float()
        target = target.cuda(args.gpu, non_blocking=True)

        # antithetic sampling
        n = images.size(0)
        t = torch.randint(
            low=0, high=args.num_timesteps, size=(n // 2 + 1,)
        ).cuda(args.gpu, non_blocking=True)
        t = torch.cat([t, args.num_timesteps - 1 - t], dim=0)[:n]

        # compute output in mode phi
        feats, yhat = model(images, target, t, mode='phi')

        # forward diffusion
        e = torch.randn_like(target).cuda(args.gpu, non_blocking=True)
        y_t = card.diffusion_utils.q_sample(target, yhat,
                                            alphas_bar_sqrt, one_minus_alphas_bar_sqrt, t, noise=e)

        output = model(feats, y_t, t, yhat, mode='theta')
        loss = criterion(e, output)

        # record loss
        losses.update(loss.item(), images.size(0))

        # compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % args.print_freq == 0:
            progress.display(i)


def validate(val_loader, model, criterion, args):
    batch_time = AverageMeter('Time', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    top1 = AverageMeter('Acc@1', ':6.2f')
    top5 = AverageMeter('Acc@5', ':6.2f')
    top1_ref = AverageMeter('Acc_ref@1', ':6.2f')
    top5_ref = AverageMeter('Acc_ref@5', ':6.2f')
    progress = ProgressMeter(
        len(val_loader),
        [batch_time, losses, top1, top5],
        prefix='Test: ')

    # switch to evaluate mode
    model.eval()

    # diffusion configs
    betas = card.diffusion_utils.make_beta_schedule(schedule="linear", num_timesteps=args.num_timesteps, 
        start=args.beta_1, end=args.beta_T).cuda(
        args.gpu, non_blocking=True)
    betas_sqrt = torch.sqrt(betas)
    alphas = 1.0 - betas
    one_minus_betas_sqrt = torch.sqrt(alphas)
    alphas_cumprod = alphas.cumprod(dim=0)
    alphas_bar_sqrt = torch.sqrt(alphas_cumprod)
    one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_cumprod)
    alphas_cumprod_prev = torch.cat(
        [torch.ones(1).cuda(args.gpu, non_blocking=True), alphas_cumprod[:-1]], dim=0
    )
    posterior_mean_coeff_1 = (
            betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)
    )
    posterior_mean_coeff_2 = (
            torch.sqrt(alphas) * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)
    )
    posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
    )

    with torch.no_grad():
        end = time.time()
        for i, (images, target) in enumerate(val_loader):
            if args.gpu is not None:
                images = images.cuda(args.gpu, non_blocking=True)
            target = target.cuda(args.gpu, non_blocking=True)

            # compute output in mode phi
            feats, yhat = model(images, target, target, mode='phi')

            # compute output
            output = card.diffusion_utils.p_sample_loop(model, feats, yhat, yhat,
                                                        args.num_timesteps, alphas,
                                                        one_minus_alphas_bar_sqrt,
                                                        only_last_sample=True)

            loss = criterion(output, nn.functional.one_hot(target, num_classes=args.num_classes).float())

            # measure accuracy and record loss
            acc1, acc5 = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), images.size(0))
            top1.update(acc1[0], images.size(0))
            top5.update(acc5[0], images.size(0))
            acc1_ref, acc5_ref = accuracy(yhat, target, topk=(1, 5))
            top1_ref.update(acc1_ref[0], images.size(0))
            top5_ref.update(acc5_ref[0], images.size(0))

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if i % args.print_freq == 0:
                progress.display(i)

        # TODO: this should also be done with the ProgressMeter
        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'
              .format(top1=top1, top5=top5))
        print(' * Ref Acc@1 {top1_ref.avg:.3f} Ref Acc@5 {top5_ref.avg:.3f}'
              .format(top1_ref=top1_ref, top5_ref=top5_ref))

    return top1.avg


def save_prediction(val_loader, model, criterion, args):
    # switch to evaluate mode
    model.eval()

    # diffusion configs
    betas = card.diffusion_utils.make_beta_schedule(schedule="linear", num_timesteps=args.num_timesteps, 
        start=args.beta_1, end=args.beta_T).cuda(
        args.gpu, non_blocking=True)
    betas_sqrt = torch.sqrt(betas)
    alphas = 1.0 - betas
    one_minus_betas_sqrt = torch.sqrt(alphas)
    alphas_cumprod = alphas.cumprod(dim=0)
    alphas_bar_sqrt = torch.sqrt(alphas_cumprod)
    one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_cumprod)
    alphas_cumprod_prev = torch.cat(
        [torch.ones(1).cuda(args.gpu, non_blocking=True), alphas_cumprod[:-1]], dim=0
    )
    posterior_mean_coeff_1 = (
            betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)
    )
    posterior_mean_coeff_2 = (
            torch.sqrt(alphas) * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)
    )
    posterior_variance = (
            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
    )

    test_labels = []
    test_preds = []
    print("=> Staring extract features and labels...")
    with torch.no_grad():
        end = time.time()
        for i, (images, target) in enumerate(val_loader):
            if args.gpu is not None:
                images = images.cuda(args.gpu, non_blocking=True)
            target = target.cuda(args.gpu, non_blocking=True)

            # compute output in mode phi
            feats, yhat = model(images, target, target, mode='phi')

            # compute output
            preds = []
            for i in range(1000):
                output = card.diffusion_utils.p_sample_loop(model, feats, yhat, yhat,
                                                            args.num_timesteps, alphas,
                                                            one_minus_alphas_bar_sqrt,
                                                            only_last_sample=True)

                preds.append(output)

            B, nc = output.shape
            tmp = torch.stack(preds).permute(1, 0, 2)
            assert tmp.shape == (B, 1000, nc)
            test_labels.append(target.cpu())
            test_preds.append(tmp.cpu())

        torch.save(torch.cat(test_labels), os.path.join(args.output_dir, f'labels_best.pth'))
        torch.save(torch.cat(test_preds), os.path.join(args.output_dir, f'preds_best.pth'))
        print("=> Finished saving at {}!".format(args.output_dir))

    return


def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):
    torch.save(state, filename)
    if is_best:
        name = filename.split('/')[-1]
        shutil.copyfile(filename, filename.replace(name, 'model_best.pth.tar'))


def sanity_check(state_dict, pretrained_weights):
    """
    Linear classifier should not change any weights other than the linear layer.
    This sanity check asserts nothing wrong happens (e.g., BN stats updated).
    """
    print("=> loading '{}' for sanity check".format(pretrained_weights))
    checkpoint = torch.load(pretrained_weights, map_location="cpu")
    state_dict_pre = checkpoint['state_dict']

    for k in list(state_dict.keys()):
        # only ignore fc layer
        if 'fc.weight' in k or 'fc.bias' in k:
            continue

        # name in pretrained model
        k_pre = 'module.' + k
        assert ((state_dict[k].cpu() == state_dict_pre[k_pre]).all()), \
            '{} is changed in linear classifier training.'.format(k)

    print("=> sanity check passed.")


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, name, fmt=':f'):
        self.name = name
        self.fmt = fmt
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def __str__(self):
        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
        return fmtstr.format(**self.__dict__)


class ProgressMeter(object):
    def __init__(self, num_batches, meters, prefix=""):
        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
        self.meters = meters
        self.prefix = prefix

    def display(self, batch):
        entries = [self.prefix + self.batch_fmtstr.format(batch)]
        entries += [str(meter) for meter in self.meters]
        print('\t'.join(entries))

    def _get_batch_fmtstr(self, num_batches):
        num_digits = len(str(num_batches // 1))
        fmt = '{:' + str(num_digits) + 'd}'
        return '[' + fmt + '/' + fmt.format(num_batches) + ']'


def adjust_learning_rate(optimizer, init_lr, epoch, args):
    """Decay the learning rate based on schedule"""
    cur_lr = init_lr * 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))
    for param_group in optimizer.param_groups:
        param_group['lr'] = cur_lr


def accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res


if __name__ == '__main__':
    args = parser.parse_args()
    print(args)
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    main(args)



================================================
FILE: classification/imagenet/card/__init__.py
================================================
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.



================================================
FILE: classification/imagenet/card/builder.py
================================================
# Built by Huangjie Zheng
# Modified from MoCo model: https://github.com/facebookresearch/moco/blob/main/moco/builder.py 

import torch
import torch.nn as nn


class CARD(nn.Module):
    """
    Build a CARD model.
    """

    def __init__(self, model_phi, num_classes=100, n_steps=1000):
        """
        num_classes: # classes for the final prediction
        n_steps: # timesteps in diffusion process (default: 1000)
        """
        super(CARD, self).__init__()

        # create prior network
        self.model_phi = model_phi(num_classes=num_classes)

        # build the Unet
        prev_dim = self.model_phi.fc.weight.shape[1]
        self.model_theta = ConditionalModel(y_dim=num_classes, feature_dim=prev_dim, n_steps=n_steps, guidance=True)

        # split prior as encoder and linear head
        self.linear = self.model_phi.fc
        self.model_phi.fc = nn.Identity()

    def forward_features(self, x):
        return self.model_phi(x)

    def forward(self, x, y, t, yhat=None, mode='theta'):
        """
        Input:
            x: data (covariates) in mode phi; feature of data in mode theta 
            y: prediction at step t
            t: timestep index
        Output:
            predicted y_t-1 from diffusion part and predicted sample from phi
        """

        # model_phi: compute features for one view and its prediction
        if mode == 'phi':
            x = self.forward_features(x)  # NxD
            yhat = self.linear(x)  # NxC
            return x, yhat.softmax(dim=-1)
        elif mode == 'theta':
            y_next = self.model_theta(x, y, t, yhat)  # NxC
            return y_next


class ConditionalLinear(nn.Module):
    def __init__(self, num_in, num_out, n_steps):
        super(ConditionalLinear, self).__init__()
        self.num_out = num_out
        self.lin = nn.Linear(num_in, num_out)
        self.embed = nn.Embedding(n_steps, num_out)
        self.embed.weight.data.uniform_()

    def forward(self, x, t):
        out = self.lin(x)
        gamma = self.embed(t)
        out = gamma.view(-1, self.num_out) * out
        return out


class ConditionalModel(nn.Module):
    def __init__(self, y_dim=100, feature_dim=2048, n_steps=1000, guidance=False):
        super(ConditionalModel, self).__init__()

        self.guidance = guidance

        # feature projection
        self.linx = nn.Linear(feature_dim, feature_dim)

        # Unet
        if self.guidance:
            self.lin1 = ConditionalLinear(y_dim * 2, feature_dim, n_steps)
        else:
            self.lin1 = ConditionalLinear(y_dim, feature_dim, n_steps)
        self.lin2 = ConditionalLinear(feature_dim, feature_dim, n_steps)
        self.lin3 = nn.Linear(feature_dim, y_dim)

    def forward(self, z, y, t, yhat=None):
        if self.guidance:
            y = torch.cat([y, yhat], dim=-1)
        y = self.lin1(y, t)
        y = self.linx(z) * y
        y = self.lin2(y, t)
        return self.lin3(y)



================================================
FILE: classification/imagenet/card/diffusion_utils.py
================================================
import math
import time
import torch
import numpy as np


def make_beta_schedule(schedule="linear", num_timesteps=1000, start=1e-5, end=1e-2):
    if schedule == "linear":
        betas = torch.linspace(start, end, num_timesteps)
    elif schedule == "const":
        betas = end * torch.ones(num_timesteps)
    elif schedule == "quad":
        betas = torch.linspace(start ** 0.5, end ** 0.5, num_timesteps) ** 2
    elif schedule == "jsd":
        betas = 1.0 / torch.linspace(num_timesteps, 1, num_timesteps)
    elif schedule == "sigmoid":
        betas = torch.linspace(-6, 6, num_timesteps)
        betas = torch.sigmoid(betas) * (end - start) + start
    elif schedule == "cosine" or schedule == "cosine_reverse":
        max_beta = 0.999
        cosine_s = 0.008
        betas = torch.tensor(
            [min(1 - (math.cos(((i + 1) / num_timesteps + cosine_s) / (1 + cosine_s) * math.pi / 2) ** 2) / (
                    math.cos((i / num_timesteps + cosine_s) / (1 + cosine_s) * math.pi / 2) ** 2), max_beta) for i in
             range(num_timesteps)])
    elif schedule == "cosine_anneal":
        betas = torch.tensor(
            [start + 0.5 * (end - start) * (1 - math.cos(t / (num_timesteps - 1) * math.pi)) for t in
             range(num_timesteps)])
    return betas


def extract(input, t, x):
    shape = x.shape
    out = torch.gather(input, 0, t.to(input.device))
    reshape = [t.shape[0]] + [1] * (len(shape) - 1)
    return out.reshape(*reshape)


# Forward functions
def q_sample(y, y_0_hat, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, t, noise=None):
    """
    y_0_hat: prediction of pre-trained guidance classifier; can be extended to represent 
        any prior mean setting at timestep T.
    """
    if noise is None:
        noise = torch.randn_like(y).to(y.device)
    sqrt_alpha_bar_t = extract(alphas_bar_sqrt, t, y)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    # q(y_t | y_0, x)
    y_t = sqrt_alpha_bar_t * y + (1 - sqrt_alpha_bar_t) * y_0_hat + sqrt_one_minus_alpha_bar_t * noise
    return y_t


# Reverse function -- sample y_{t-1} given y_t
def p_sample(model, x, y, y_0_hat, y_T_mean, t, alphas, one_minus_alphas_bar_sqrt):
    """
    Reverse diffusion process sampling -- one time step.

    y: sampled y at time step t, y_t.
    y_0_hat: prediction of pre-trained guidance model.
    y_T_mean: mean of prior distribution at timestep T.
    We replace y_0_hat with y_T_mean in the forward process posterior mean computation, emphasizing that 
        guidance model prediction y_0_hat = f_phi(x) is part of the input to eps_theta network, while 
        in paper we also choose to set the prior mean at timestep T y_T_mean = f_phi(x).
    """
    device = next(model.parameters()).device
    z = torch.randn_like(y)
    t = torch.tensor([t]).to(device)
    alpha_t = extract(alphas, t, y)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_one_minus_alpha_bar_t_m_1 = extract(one_minus_alphas_bar_sqrt, t - 1, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    sqrt_alpha_bar_t_m_1 = (1 - sqrt_one_minus_alpha_bar_t_m_1.square()).sqrt()
    # y_t_m_1 posterior mean component coefficients
    gamma_0 = (1 - alpha_t) * sqrt_alpha_bar_t_m_1 / (sqrt_one_minus_alpha_bar_t.square())
    gamma_1 = (sqrt_one_minus_alpha_bar_t_m_1.square()) * (alpha_t.sqrt()) / (sqrt_one_minus_alpha_bar_t.square())
    gamma_2 = 1 + (sqrt_alpha_bar_t - 1) * (alpha_t.sqrt() + sqrt_alpha_bar_t_m_1) / (
        sqrt_one_minus_alpha_bar_t.square())
    eps_theta = model(x, y, t, yhat=y_0_hat, mode='theta').to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t)
    # posterior mean
    y_t_m_1_hat = gamma_0 * y_0_reparam + gamma_1 * y + gamma_2 * y_T_mean
    # posterior variance
    beta_t_hat = (sqrt_one_minus_alpha_bar_t_m_1.square()) / (sqrt_one_minus_alpha_bar_t.square()) * (1 - alpha_t)
    y_t_m_1 = y_t_m_1_hat.to(device) + beta_t_hat.sqrt().to(device) * z.to(device)
    return y_t_m_1


# Reverse function -- sample y_0 given y_1
def p_sample_t_1to0(model, x, y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt):
    device = next(model.parameters()).device
    t = torch.tensor([0]).to(device)  # corresponding to timestep 1 (i.e., t=1 in diffusion models)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    eps_theta = model(x, y, t, y_0_hat, mode='theta').to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t).to(device)
    return y_0_reparam


def y_0_reparam(model, x, y, y_0_hat, y_T_mean, t, one_minus_alphas_bar_sqrt):
    """
    Obtain y_0 reparameterization from q(y_t | y_0), in which noise term is the eps_theta prediction.
    Algorithm 2 Line 4 in paper.
    """
    device = next(model.parameters()).device
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    eps_theta = model(x, y, t, y_0_hat, mode='theta').to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t).to(device)
    return y_0_reparam


def p_sample_loop(model, x, y_0_hat, y_T_mean, n_steps, alphas, one_minus_alphas_bar_sqrt,
                  only_last_sample=False):
    num_t, y_p_seq = None, None
    device = next(model.parameters()).device
    z = torch.randn_like(y_T_mean).to(device)
    cur_y = z + y_T_mean  # sampled y_T
    if only_last_sample:
        num_t = 1
    else:
        y_p_seq = [cur_y]
    for t in reversed(range(1, n_steps)):
        y_t = cur_y
        cur_y = p_sample(model, x, y_t, y_0_hat, y_T_mean, t, alphas, one_minus_alphas_bar_sqrt)  # y_{t-1}
        if only_last_sample:
            num_t += 1
        else:
            y_p_seq.append(cur_y)
    if only_last_sample:
        assert num_t == n_steps
        y_0 = p_sample_t_1to0(model, x, cur_y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt)
        return y_0
    else:
        assert len(y_p_seq) == n_steps
        y_0 = p_sample_t_1to0(model, x, y_p_seq[-1], y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt)
        y_p_seq.append(y_0)
        return y_p_seq



================================================
FILE: classification/pretraining/encoder.py
================================================
import torch
import torch.nn as nn
from torchvision.models.resnet import resnet18, resnet50


# adapted from https://colab.research.google.com/github/kjamithash/Pytorch_DeepLearning_Experiments/blob/master
# /FashionMNIST_ResNet_TransferLearning.ipynb
class ResNetEmbedding(nn.Module):
    def __init__(self, arch, in_channels=1):
        super(ResNetEmbedding, self).__init__()

        # Load a pretrained resnet model from torchvision.models in Pytorch
        if arch == 'resnet50':
            self.model = resnet50(pretrained=True)
        elif arch == 'resnet18':
            self.model = resnet18(pretrained=True)

        if in_channels != 3:
            self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)

        self.featdim = self.model.fc.in_features
        # # Change the output layer to output 10 classes instead of 1000 classes
        # self.model.fc = nn.Linear(self.featdim, 10)

    def forward(self, x):
        return self.model(x)


class Model(nn.Module):
    def __init__(self, config):
        super(Model, self).__init__()

        self.f = []
        num_class = config.data.num_classes
        arch = 'resnet18'
        in_channels = 1 if config.data.dataset != "CIFAR10" else 3
        backbone = ResNetEmbedding(arch, in_channels)
        # obtain NN modules before the output linear layer
        for name, module in backbone.model.named_children():
            if name != 'fc':
                self.f.append(module)

        # encoder
        self.f = nn.Sequential(*self.f)
        # projection head
        self.featdim = backbone.featdim
        # output linear layer without bias, as class prototype matrix
        self.g = nn.Linear(self.featdim, num_class, bias=False)

    def forward_feature(self, x):
        x = self.f(x)
        feature = torch.flatten(x, start_dim=1)
        return feature

    def forward(self, x, return_feature=False):
        feature = self.forward_feature(x)
        out = self.g(feature)
        if return_feature:
            return feature, out
        else:
            return out




================================================
FILE: classification/pretraining/resnet.py
================================================
import torch.nn as nn
import torch.nn.functional as F


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None,
                 groups=1, base_width=64, dilation=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None,
                 groups=1, base_width=64, dilation=1):
        super(Bottleneck, self).__init__()
        width = int(planes * (base_width / 64.)) * groups
        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1,
                               stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width)
        self.conv2 = nn.Conv2d(width, width, kernel_size=3, padding=1,
                               stride=stride, groups=groups,
                               dilation=dilation, bias=False)
        self.bn2 = nn.BatchNorm2d(width)
        self.conv3 = nn.Conv2d(width, planes * self.expansion,
                               kernel_size=1, stride=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes=10,
                 groups=1, width_per_group=64):
        super(ResNet, self).__init__()
        self.in_planes = 64
        self.dilation = 1
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(self.in_planes)
        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        downsample = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_planes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion)
            )
        layers = list()
        layers.append(block(self.in_planes, planes, stride, downsample))
        self.in_planes = planes * block.expansion
        for _ in range(1, num_blocks):
            layers.append(block(self.in_planes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out


def build_ResNet(cfg):
    _blocks = {
        "Basic": BasicBlock,
        "Bottleneck": Bottleneck,
    }
    return ResNet(_blocks[cfg['RESNET_BLOCK']],
                  cfg['RESNET_LAYERS'], cfg['NUM_CLASSES'])


def ResNet18(num_classes=10):
    cfg ={
        "RESNET_BLOCK": "Basic",
        "RESNET_LAYERS": (2, 2, 2, 2),
        "NUM_CLASSES": num_classes,
    }
    return build_ResNet(cfg)



================================================
FILE: classification/training_scripts/run_cifar10.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export RUN_NAME=run_1
export PRIOR_TYPE=f_phi_prior
export CAT_F_PHI=_cat_f_phi
export F_PHI_TYPE=f_phi_supervised  #f_phi_self_supervised
export MODEL_VERSION_DIR=card_onehot_conditional_results/${N_STEPS}steps/nn/${RUN_NAME}/${PRIOR_TYPE}${CAT_F_PHI}/${F_PHI_TYPE}
export LOSS=card_onehot_conditional
export TASK=cifar10
export N_SPLITS=1
export DEVICE_ID=0
export N_THREADS=8

python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --config configs/${TASK}.yml --exp $EXP_DIR/${MODEL_VERSION_DIR} --doc ${TASK} --n_splits ${N_SPLITS}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --exp $EXP_DIR/${MODEL_VERSION_DIR} --doc ${TASK} --n_splits ${N_SPLITS} --test --tune_T



================================================
FILE: regression/data_loader.py
================================================
import torch
import os
import utils
import numpy as np
import pandas as pd
import abc
from sklearn.datasets import make_swiss_roll, make_moons
from sklearn.preprocessing import StandardScaler


class SwissRoll:
    """
    Swiss roll distribution sampler.
    noise control the amount of noise injected to make a thicker swiss roll
    """

    def sample(self, n, noise=0.5):
        if noise is None:
            noise = 0.5
        return torch.from_numpy(
            make_swiss_roll(n_samples=n, noise=noise)[0][:, [0, 2]].astype('float32') / 5.)


class Moons:
    """
    Double moons distribution sampler.
    noise control the amount of noise injected to make a thicker swiss roll
    """

    def sample(self, n, noise=0.02):
        if noise is None:
            noise = 0.02
        temp = make_moons(n_samples=n, noise=noise)[0].astype('float32')
        return torch.from_numpy(temp / abs(temp).max())


class Gaussians:
    """
    Gaussian mixture distribution sampler.
    noise control the amount of noise injected to make a thicker swiss roll
    """

    def sample(self, n, noise=0.02, mode=8):
        if noise is None:
            noise = 0.02

        if mode == 8:
            scale = 2.
            centers = [
                (1, 0), (-1, 0), (0, 1), (0, -1),
                (1. / np.sqrt(2), 1. / np.sqrt(2)), (1. / np.sqrt(2), -1. / np.sqrt(2)),
                (-1. / np.sqrt(2), 1. / np.sqrt(2)), (-1. / np.sqrt(2), -1. / np.sqrt(2))
            ]
            centers = [(scale * x, scale * y) for x, y in centers]
            temp = []
            labels = []
            for i in range(n):
                point = np.random.randn(2) * .02
                label = np.random.choice(np.arange(len(centers)))
                center = centers[label]
                point[0] += center[0]
                point[1] += center[1]
                temp.append(point)
                labels.append(label)
            temp = np.array(temp, dtype='float32')
            labels = np.array(labels)
            temp /= 1.414  # stdev
        elif mode == 25:
            temp = []
            labels = []
            for i in range(int(n / 25)):
                label = 0
                for x in range(-2, 3):
                    for y in range(-2, 3):
                        point = np.random.randn(2) * 0.05
                        point[0] += 2 * x
                        point[1] += 2 * y
                        temp.append(point)
                        labels.append(label)
                        label += 1
            temp = np.array(temp, dtype='float32')
            labels = np.array(labels)
            rand_idx = np.arange(n)
            np.random.shuffle(rand_idx)
            temp = temp[rand_idx] / 2.828  # stdev
            labels = labels[rand_idx]
        return torch.from_numpy(temp)


def _get_index_train_test_path(data_directory_path, split_num, train=True):
    """
       Method to generate the path containing the training/test split for the given
       split number (generally from 1 to 20).
       @param split_num      Split number for which the data has to be generated
       @param train          Is true if the data is training data. Else false.
       @return path          Path of the file containing the requried data
    """
    if train:
        return os.path.join(data_directory_path, "index_train_" + str(split_num) + ".txt")
    else:
        return os.path.join(data_directory_path, "index_test_" + str(split_num) + ".txt")


def onehot_encode_cat_feature(X, cat_var_idx_list):
    """
    Apply one-hot encoding to the categorical variable(s) in the feature set,
        specified by the index list.
    """
    # select numerical features
    X_num = np.delete(arr=X, obj=cat_var_idx_list, axis=1)
    # select categorical features
    X_cat = X[:, cat_var_idx_list]
    X_onehot_cat = []
    for col in range(X_cat.shape[1]):
        X_onehot_cat.append(pd.get_dummies(X_cat[:, col], drop_first=True))
    X_onehot_cat = np.concatenate(X_onehot_cat, axis=1).astype(np.float32)
    dim_cat = X_onehot_cat.shape[1]  # number of categorical feature(s)
    X = np.concatenate([X_num, X_onehot_cat], axis=1)
    return X, dim_cat


def preprocess_uci_feature_set(X, config):
    """
    Obtain preprocessed UCI feature set X (one-hot encoding applied for categorical variable)
        and dimension of one-hot encoded categorical variables.
    """
    dim_cat = 0
    task_name = config.data.dir
    if config.data.one_hot_encoding:
        if task_name == 'bostonHousing':
            X, dim_cat = onehot_encode_cat_feature(X, [3])
        elif task_name == 'energy':
            X, dim_cat = onehot_encode_cat_feature(X, [4, 6, 7])
        elif task_name == 'naval-propulsion-plant':
            X, dim_cat = onehot_encode_cat_feature(X, [0, 1, 8, 11])
        else:
            pass
    return X, dim_cat


############################
### UCI regression tasks ###
class UCI_Dataset(object):
    def __init__(self, config, split, validation=False):
        # global variables for reading data files
        _DATA_DIRECTORY_PATH = os.path.join(config.data.data_root, config.data.dir, "data")
        _DATA_FILE = os.path.join(_DATA_DIRECTORY_PATH, "data.txt")
        _INDEX_FEATURES_FILE = os.path.join(_DATA_DIRECTORY_PATH, "index_features.txt")
        _INDEX_TARGET_FILE = os.path.join(_DATA_DIRECTORY_PATH, "index_target.txt")
        _N_SPLITS_FILE = os.path.join(_DATA_DIRECTORY_PATH, "n_splits.txt")

        # set random seed 1 -- same setup as MC Dropout
        utils.set_random_seed(1)

        # load the data
        data = np.loadtxt(_DATA_FILE)
        # load feature and target indices
        index_features = np.loadtxt(_INDEX_FEATURES_FILE)
        index_target = np.loadtxt(_INDEX_TARGET_FILE)
        # load feature and target as X and y
        X = data[:, [int(i) for i in index_features.tolist()]].astype(np.float32)
        y = data[:, int(index_target.tolist())].astype(np.float32)
        # preprocess feature set X
        X, dim_cat = preprocess_uci_feature_set(X=X, config=config)
        self.dim_cat = dim_cat

        # load the indices of the train and test sets
        index_train = np.loadtxt(_get_index_train_test_path(_DATA_DIRECTORY_PATH, split, train=True))
        index_test = np.loadtxt(_get_index_train_test_path(_DATA_DIRECTORY_PATH, split, train=False))

        # read in data files with indices
        x_train = X[[int(i) for i in index_train.tolist()]]
        y_train = y[[int(i) for i in index_train.tolist()]].reshape(-1, 1)
        x_test = X[[int(i) for i in index_test.tolist()]]
        y_test = y[[int(i) for i in index_test.tolist()]].reshape(-1, 1)

        # split train set further into train and validation set for hyperparameter tuning
        if validation:
            num_training_examples = int(config.diffusion.nonlinear_guidance.train_ratio * x_train.shape[0])
            x_test = x_train[num_training_examples:, :]
            y_test = y_train[num_training_examples:]
            x_train = x_train[0:num_training_examples, :]
            y_train = y_train[0:num_training_examples]

        self.x_train = x_train if type(x_train) is torch.Tensor else torch.from_numpy(x_train)
        self.y_train = y_train if type(y_train) is torch.Tensor else torch.from_numpy(y_train)
        self.x_test = x_test if type(x_test) is torch.Tensor else torch.from_numpy(x_test)
        self.y_test = y_test if type(y_test) is torch.Tensor else torch.from_numpy(y_test)

        self.train_n_samples = x_train.shape[0]
        self.train_dim_x = self.x_train.shape[1]  # dimension of training data input
        self.train_dim_y = self.y_train.shape[1]  # dimension of training regression output

        self.test_n_samples = x_test.shape[0]
        self.test_dim_x = self.x_test.shape[1]  # dimension of testing data input
        self.test_dim_y = self.y_test.shape[1]  # dimension of testing regression output

        self.normalize_x = config.data.normalize_x
        self.normalize_y = config.data.normalize_y
        self.scaler_x, self.scaler_y = None, None

        if self.normalize_x:
            self.normalize_train_test_x()
        if self.normalize_y:
            self.normalize_train_test_y()

    def normalize_train_test_x(self):
        """
        When self.dim_cat > 0, we have one-hot encoded number of categorical variables,
            on which we don't conduct standardization. They are arranged as the last
            columns of the feature set.
        """
        self.scaler_x = StandardScaler(with_mean=True, with_std=True)
        if self.dim_cat == 0:
            self.x_train = torch.from_numpy(
                self.scaler_x.fit_transform(self.x_train).astype(np.float32))
            self.x_test = torch.from_numpy(
                self.scaler_x.transform(self.x_test).astype(np.float32))
        else:  # self.dim_cat > 0
            x_train_num, x_train_cat = self.x_train[:, :-self.dim_cat], self.x_train[:, -self.dim_cat:]
            x_test_num, x_test_cat = self.x_test[:, :-self.dim_cat], self.x_test[:, -self.dim_cat:]
            x_train_num = torch.from_numpy(
                self.scaler_x.fit_transform(x_train_num).astype(np.float32))
            x_test_num = torch.from_numpy(
                self.scaler_x.transform(x_test_num).astype(np.float32))
            self.x_train = torch.from_numpy(np.concatenate([x_train_num, x_train_cat], axis=1))
            self.x_test = torch.from_numpy(np.concatenate([x_test_num, x_test_cat], axis=1))

    def normalize_train_test_y(self):
        self.scaler_y = StandardScaler(with_mean=True, with_std=True)
        self.y_train = torch.from_numpy(
            self.scaler_y.fit_transform(self.y_train).astype(np.float32))
        self.y_test = torch.from_numpy(
            self.scaler_y.transform(self.y_test).astype(np.float32))

    def return_dataset(self, split="train"):
        if split == "train":
            train_dataset = torch.cat((self.x_train, self.y_train), dim=1)
            return train_dataset
        else:
            test_dataset = torch.cat((self.x_test, self.y_test), dim=1)
            return test_dataset

    def summary_dataset(self, split="train"):
        if split == "train":
            return {'n_samples': self.train_n_samples, 'dim_x': self.train_dim_x, 'dim_y': self.train_dim_y}
        else:
            return {'n_samples': self.test_n_samples, 'dim_x': self.test_dim_x, 'dim_y': self.test_dim_y}


def compute_y_noiseless_mean(dataset, x_test_batch, true_function='linear'):
    """
    Compute the mean of y with the ground truth data generation function.
    """
    if true_function == 'linear':
        y_true_mean = dataset.a + dataset.b * x_test_batch
    elif true_function == 'quadratic':
        y_true_mean = dataset.a * x_test_batch.pow(2) + dataset.b * x_test_batch + dataset.c
    elif true_function == 'loglinear':
        y_true_mean = (dataset.a + dataset.b * x_test_batch).exp()
    elif true_function == 'loglog':
        y_true_mean = (np.log(dataset.a) + dataset.b * x_test_batch.log()).exp()
    elif true_function == 'mdnsinusoidal':
        y_true_mean = x_test_batch + 0.3 * torch.sin(2 * np.pi * x_test_batch)
    elif true_function == 'sinusoidal':
        y_true_mean = x_test_batch * torch.sin(x_test_batch)
    else:
        raise NotImplementedError('We don\'t have such data generation scheme for toy example.')
    return y_true_mean.numpy()


class Dataset(object):
    def __init__(self, seed, n_samples):
        self.seed = seed
        self.n_samples = n_samples
        self.eps_samples = None
        utils.set_random_seed(self.seed)

    @abc.abstractmethod
    def create_train_test_dataset(self):
        pass

    def create_noises(self, noise_dict):
        """
        Ref: https://pytorch.org/docs/stable/distributions.html
        :param noise_dict: {"noise_type": "norm", "loc": 0., "scale": 1.}
        """
        print("Create noises using the following parameters:")
        print(noise_dict)
        noise_type = noise_dict.get("noise_type", "norm")
        if noise_type == "t":
            dist = torch.distributions.studentT.StudentT(df=noise_dict.get("df", 10.), loc=noise_dict.get("loc", 0.0),
                                                         scale=noise_dict.get("scale", 1.0))
        elif noise_type == "unif":
            dist = torch.distributions.uniform.Uniform(low=noise_dict.get("low", 0.), high=noise_dict.get("high", 1.))
        elif noise_type == "Chi2":
            dist = torch.distributions.chi2.Chi2(df=noise_dict.get("df", 10.))
        elif noise_type == "Laplace":
            dist = torch.distributions.laplace.Laplace(loc=noise_dict.get("loc", 0.), scale=noise_dict.get("scale", 1.))
        else:  # noise_type == "norm"
            dist = torch.distributions.normal.Normal(loc=noise_dict.get("loc", 0.), scale=noise_dict.get("scale", 1.))

        self.eps_samples = dist.sample((self.n_samples, 1))


class DatasetWithOneX(Dataset):
    def __init__(self, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        super(DatasetWithOneX, self).__init__(seed=seed, n_samples=n_samples)
        self.x_dict = x_dict
        self.x_samples = self.sample_x(self.x_dict)
        self.dim_x = self.x_samples.shape[1]  # dimension of data input
        self.y = self.create_y_from_one_x(noise_dict=noise_dict)
        self.dim_y = self.y.shape[1]  # dimension of regression output
        self.x_train, self.y_train, self.x_test, self.y_test = None, None, None, None
        self.train_n_samples, self.test_n_samples = None, None
        self.train_dataset, self.test_dataset = None, None
        self.normalize_x = normalize_x
        self.normalize_y = normalize_y
        self.scaler_x, self.scaler_y = None, None

    def sample_x(self, x_dict):
        """
        :param x_dict: {"dist_type": "unif", "low": 0., "high": 1.}
        """
        print("Create x using the following parameters:")
        print(x_dict)
        dist_type = x_dict.get("dist_type", "unif")
        if dist_type == "norm":
            dist = torch.distributions.normal.Normal(loc=x_dict.get("loc", 0.), scale=x_dict.get("scale", 1.))
        else:
            dist = torch.distributions.uniform.Uniform(low=x_dict.get("low", 0.), high=x_dict.get("high", 1.))

        return dist.sample((self.n_samples, 1))

    def create_y_from_one_x(self, noise_dict):
        if self.eps_samples is None:
            n_samples_temp = self.n_samples
            if type(noise_dict.get("scale", 1.)) == torch.Tensor:
                self.n_samples = 1
            self.create_noises(noise_dict)
            if self.n_samples == 1:
                self.eps_samples = self.eps_samples[0]
                self.n_samples = n_samples_temp

    def create_train_test_dataset(self, train_ratio=0.8):
        utils.set_random_seed(self.seed)
        data_idx = np.arange(self.n_samples)
        np.random.shuffle(data_idx)
        train_size = int(self.n_samples * train_ratio)
        self.x_train, self.y_train, self.x_test, self.y_test = \
            self.x_samples[data_idx[:train_size]], self.y[data_idx[:train_size]], \
            self.x_samples[data_idx[train_size:]], self.y[data_idx[train_size:]]
        self.train_n_samples = self.x_train.shape[0]
        self.test_n_samples = self.x_test.shape[0]
        # standardize x and y if needed
        if self.normalize_x:
            self.normalize_train_test_x()
        if self.normalize_y:
            self.normalize_train_test_y()
        self.train_dataset = torch.cat((self.x_train, self.y_train), dim=1)
        # sort x for easier plotting purpose during test time
        if self.dim_x == 1:
            sorted_idx = torch.argsort(self.x_test, dim=0).squeeze()
            self.x_test = self.x_test[sorted_idx]
            self.y_test = self.y_test[sorted_idx]
        self.test_dataset = torch.cat((self.x_test, self.y_test), dim=1)

    def normalize_train_test_x(self):
        self.scaler_x = StandardScaler(with_mean=True, with_std=True)
        self.x_train = torch.from_numpy(
            self.scaler_x.fit_transform(self.x_train).astype(np.float32))
        self.x_test = torch.from_numpy(
            self.scaler_x.transform(self.x_test).astype(np.float32))

    def normalize_train_test_y(self):
        self.scaler_y = StandardScaler(with_mean=True, with_std=True)
        self.y_train = torch.from_numpy(
            self.scaler_y.fit_transform(self.y_train).astype(np.float32))
        self.y_test = torch.from_numpy(
            self.scaler_y.transform(self.y_test).astype(np.float32))


class LinearDatasetWithOneX(DatasetWithOneX):
    def __init__(self, a, b, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        self.a = a
        self.b = b
        super(LinearDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        super().create_y_from_one_x(noise_dict)
        y = self.a + self.b * self.x_samples + self.eps_samples
        return y


class QuadraticDatasetWithOneX(DatasetWithOneX):
    def __init__(self, a, b, c, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        self.a = a
        self.b = b
        self.c = c
        super(QuadraticDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        super().create_y_from_one_x(noise_dict)
        y = self.a * self.x_samples.pow(2) + self.b * self.x_samples + self.c + self.eps_samples
        return y


class CircleDatasetWithOneX(DatasetWithOneX):
    def __init__(self, r, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        self.r = r
        self.theta = torch.rand((n_samples, 1)) * 2 * np.pi
        x_dict.update(r=r)
        super(CircleDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y
        )

    def create_y_from_one_x(self, noise_dict):
        super().create_y_from_one_x(noise_dict)
        r_samples = self.r + self.eps_samples
        self.x_samples, y = r_samples * torch.cos(self.theta), r_samples * torch.sin(self.theta)
        self.dim_x = self.x_samples.shape[1]
        return y


class LogLinearDatasetWithOneX(DatasetWithOneX):
    def __init__(self, a, b, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        self.a = a
        self.b = b
        super(LogLinearDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        # log(y) = a + b * x + eps
        super().create_y_from_one_x(noise_dict)
        logy = self.a + self.b * self.x_samples + self.eps_samples
        return logy.exp()


class LogLogDatasetWithOneX(DatasetWithOneX):
    def __init__(self, a, b, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        self.a = a
        self.b = b
        super(LogLogDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        # log(y) = log(a) + b * log(x) + eps
        super().create_y_from_one_x(noise_dict)
        logy = np.log(self.a) + self.b * self.x_samples.log() + self.eps_samples
        return logy.exp()


class LinearDatasetWithStdIncreaseWithX(LinearDatasetWithOneX):
    def __init__(self, a, b, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        super(LinearDatasetWithStdIncreaseWithX, self).__init__(
            a=a, b=b, n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        # std of eps increases with abs(x), fix eps to be normal noises
        noise_dict.update(noise_type="norm", scale=noise_dict.get("scale", 1.) * self.x_samples.abs())
        return super().create_y_from_one_x(noise_dict)


class SinusoidDatasetWithOneX(DatasetWithOneX):
    """
    Both "Snelson" Dataset and "OAT-1D" Dataset are sinusoid curve.
    Currently use function from Fig. 4 of Mixture Density Networks.
    """

    def __init__(self, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        super(SinusoidDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        # y = x + 0.3 * sin(2 * pi * x) + eps
        super().create_y_from_one_x(noise_dict)
        y = self.x_samples + 0.3 * torch.sin(2 * np.pi * self.x_samples) + self.eps_samples
        return y

    def invert_xy(self):
        """
        Swap x and y to have a one-to-many mapping, like in MDN paper.
        """
        temp_x = self.y
        temp_dim_x = self.dim_y
        self.y = self.x_samples
        self.dim_y = self.dim_x
        self.x_samples = temp_x
        self.dim_x = temp_dim_x


class SubspaceInferenceDatasetWithOneX(DatasetWithOneX):
    def __init__(self, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        self.x_ranges = [[-7.2, -4.8], [-1.2, 1.2], [4.8, 7.2]]
        x_dict.update(x_ranges=self.x_ranges)
        super(SubspaceInferenceDatasetWithOneX, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y)

    def create_y_from_one_x(self, noise_dict):
        """
        Ref: https://arxiv.org/pdf/1907.07504.pdf, Section 5.1
        The network takes two inputs, x and x ** 2
        """
        super().create_y_from_one_x(noise_dict)
        x_ranges_len = [x[1] - x[0] for x in self.x_ranges]
        x_ranges_total_len = sum(x_ranges_len)
        x_ranges_num_samples = [int(x / x_ranges_total_len * self.n_samples) for x in x_ranges_len]
        self.x_samples = torch.tensor([])
        for idx, x_range in enumerate(self.x_ranges):
            self.x_samples = torch.cat([
                self.x_samples,
                torch.distributions.uniform.Uniform(
                    low=self.x_ranges[idx][0], high=self.x_ranges[idx][1]).sample((x_ranges_num_samples[idx], 1))
            ], axis=0)
        self.dim_x = self.x_samples.shape[1]
        y = utils.SubspaceInferenceDatasetNet().forward(
            x=torch.cat([self.x_samples, self.x_samples ** 2], axis=1)) + self.eps_samples
        return y


class CTToyDataset(DatasetWithOneX):
    def __init__(self, n_samples, seed, x_dict, noise_dict, normalize_x=False, normalize_y=False):
        super(CTToyDataset, self).__init__(
            n_samples=n_samples, seed=seed, x_dict=x_dict, noise_dict=noise_dict,
            normalize_x=normalize_x, normalize_y=normalize_y
        )

    def create_y_from_one_x(self, noise_dict):
        super().create_y_from_one_x(noise_dict)
        X = self.load_data(name=self.x_dict.get("ct_toy_name", '8gaussians'))
        self.x_samples = X[:, :1]
        self.dim_x = self.x_samples.shape[1]
        y = X[:, 1:]  # + self.eps_samples
        # TODO: do we need adding noise to y
        return y

    def load_data(self, name):
        N = self.n_samples
        if name == 'swiss_roll':
            temp = make_swiss_roll(n_samples=N, noise=0.05)[0][:, (0, 2)]
            temp /= abs(temp).max()
        elif name == 'half_moons':
            temp = make_moons(n_samples=N, noise=0.02)[0]
            temp /= abs(temp).max()
        elif name == '2gaussians':
            scale = 2.
            centers = [
                (1. / np.sqrt(2), 1. / np.sqrt(2)), (-1. / np.sqrt(2), -1. / np.sqrt(2))
            ]
            centers = [(scale * x, scale * y) for x, y in centers]
            temp = []
            for i in range(N):
                point = np.random.randn(2) * .02
                center = centers[np.random.choice(np.arange(len(centers)))]
                point[0] += center[0]
                point[1] += center[1]
                temp.append(point)
            temp = np.array(temp, dtype='float32')
            temp /= 1.414  # stdev
        elif name == '8gaussians':
            scale = 2.
            centers = [
                (1, 0), (-1, 0), (0, 1), (0, -1),
                (1. / np.sqrt(2), 1. / np.sqrt(2)), (1. / np.sqrt(2), -1. / np.sqrt(2)),
                (-1. / np.sqrt(2), 1. / np.sqrt(2)), (-1. / np.sqrt(2), -1. / np.sqrt(2))
            ]
            centers = [(scale * x, scale * y) for x, y in centers]
            temp = []
            for i in range(N):
                point = np.random.randn(2) * .1  # .02
                center = centers[np.random.choice(np.arange(len(centers)))]
                point[0] += center[0]
                point[1] += center[1]
                temp.append(point)
            temp = np.array(temp, dtype='float32')
            temp /= 1.414  # stdev
        elif name == '25gaussians':
            temp = []
            for i in range(int(N / 25)):
                for x in range(-2, 3):
                    for y in range(-2, 3):
                        point = np.random.randn(2) * 0.05
                        point[0] += 2 * x
                        point[1] += 2 * y
                        temp.append(point)
            temp = np.array(temp, dtype='float32')
            np.random.shuffle(temp)
            temp /= 2.828  # stdev
        elif name == 'circle':
            temp, y = make_circles(n_samples=N, noise=0.05)
            temp = temp[np.argwhere(y == 0).squeeze(), :]
        elif name == 's_curve':
            temp = make_s_curve(n_samples=N, noise=0.02)[0]  # n_samples=500
            temp = np.stack([temp[:, 0], temp[:, 2]], axis=1)
        else:
            raise Exception(
                ("Dataset not found: name must be 'swiss_roll', 'half_moons', " +
                 "'circle', 's_curve', '8gaussians' or '25gaussians'."))
        X = torch.from_numpy(temp).float()
        return X


if __name__ == '__main__':
    if not os.path.exists('./data'):
        os.makedirs('./data')

    import matplotlib.pyplot as plt

    sampler = SwissRoll()
    x = sampler.sample(10000).data.numpy()
    plt.close('all')
    fig = plt.figure(figsize=(5, 5))
    _ = plt.hist2d(x[:, 0], x[:, 1], 200, )
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(os.path.join('data', 'swiss_roll.pdf'))

    sampler = Moons()
    x = sampler.sample(10000).data.numpy()
    plt.close('all')
    fig = plt.figure(figsize=(5, 5))
    _ = plt.hist2d(x[:, 0], x[:, 1], 200, )
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(os.path.join('data', 'moons.pdf'))

    sampler = Gaussians()
    x = sampler.sample(10000, mode=8).data.numpy()
    plt.close('all')
    fig = plt.figure(figsize=(5, 5))
    _ = plt.hist2d(x[:, 0], x[:, 1], 200, )
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(os.path.join('data', '8gaussians.pdf'))

    sampler = Gaussians()
    x = sampler.sample(10000, mode=25).data.numpy()
    plt.close('all')
    fig = plt.figure(figsize=(5, 5))
    _ = plt.hist2d(x[:, 0], x[:, 1], 200, )
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(os.path.join('data', '25gaussians.pdf'))



================================================
FILE: regression/diffusion_utils.py
================================================
import math
import torch
import numpy as np


def make_beta_schedule(schedule="linear", num_timesteps=1000, start=1e-5, end=1e-2):
    if schedule == "linear":
        betas = torch.linspace(start, end, num_timesteps)
    elif schedule == "const":
        betas = end * torch.ones(num_timesteps)
    elif schedule == "quad":
        betas = torch.linspace(start ** 0.5, end ** 0.5, num_timesteps) ** 2
    elif schedule == "jsd":
        betas = 1.0 / torch.linspace(num_timesteps, 1, num_timesteps)
    elif schedule == "sigmoid":
        betas = torch.linspace(-6, 6, num_timesteps)
        betas = torch.sigmoid(betas) * (end - start) + start
    elif schedule == "cosine" or schedule == "cosine_reverse":
        max_beta = 0.999
        cosine_s = 0.008
        betas = torch.tensor(
            [min(1 - (math.cos(((i + 1) / num_timesteps + cosine_s) / (1 + cosine_s) * math.pi / 2) ** 2) / (
                    math.cos((i / num_timesteps + cosine_s) / (1 + cosine_s) * math.pi / 2) ** 2), max_beta) for i in
             range(num_timesteps)])
        if schedule == "cosine_reverse":
            betas = betas.flip(0)  # starts at max_beta then decreases fast
    elif schedule == "cosine_anneal":
        betas = torch.tensor(
            [start + 0.5 * (end - start) * (1 - math.cos(t / (num_timesteps - 1) * math.pi)) for t in
             range(num_timesteps)])
    return betas


def extract(input, t, x):
    shape = x.shape
    out = torch.gather(input, 0, t.to(input.device))
    reshape = [t.shape[0]] + [1] * (len(shape) - 1)
    return out.reshape(*reshape)


# Forward functions
def q_sample(y, y_0_hat, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, t, noise=None):
    """
    y_0_hat: prediction of pre-trained guidance model; can be extended to represent
        any prior mean setting at timestep T.
    """
    if noise is None:
        noise = torch.randn_like(y).to(y.device)
    sqrt_alpha_bar_t = extract(alphas_bar_sqrt, t, y)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    # q(y_t | y_0, x)
    y_t = sqrt_alpha_bar_t * y + (1 - sqrt_alpha_bar_t) * y_0_hat + sqrt_one_minus_alpha_bar_t * noise
    return y_t


# Reverse function -- sample y_{t-1} given y_t
def p_sample(model, x, y, y_0_hat, y_T_mean, t, alphas, one_minus_alphas_bar_sqrt):
    """
    Reverse diffusion process sampling -- one time step.

    y: sampled y at time step t, y_t.
    y_0_hat: prediction of pre-trained guidance model.
    y_T_mean: mean of prior distribution at timestep T.
    We replace y_0_hat with y_T_mean in the forward process posterior mean computation, emphasizing that 
        guidance model prediction y_0_hat = f_phi(x) is part of the input to eps_theta network, while 
        in paper we also choose to set the prior mean at timestep T y_T_mean = f_phi(x).
    """
    device = next(model.parameters()).device
    z = torch.randn_like(y)  # if t > 1 else torch.zeros_like(y)
    t = torch.tensor([t]).to(device)
    alpha_t = extract(alphas, t, y)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_one_minus_alpha_bar_t_m_1 = extract(one_minus_alphas_bar_sqrt, t - 1, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    sqrt_alpha_bar_t_m_1 = (1 - sqrt_one_minus_alpha_bar_t_m_1.square()).sqrt()
    # y_t_m_1 posterior mean component coefficients
    gamma_0 = (1 - alpha_t) * sqrt_alpha_bar_t_m_1 / (sqrt_one_minus_alpha_bar_t.square())
    gamma_1 = (sqrt_one_minus_alpha_bar_t_m_1.square()) * (alpha_t.sqrt()) / (sqrt_one_minus_alpha_bar_t.square())
    gamma_2 = 1 + (sqrt_alpha_bar_t - 1) * (alpha_t.sqrt() + sqrt_alpha_bar_t_m_1) / (
        sqrt_one_minus_alpha_bar_t.square())
    eps_theta = model(x, y, y_0_hat, t).to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t)
    # posterior mean
    y_t_m_1_hat = gamma_0 * y_0_reparam + gamma_1 * y + gamma_2 * y_T_mean
    # posterior variance
    beta_t_hat = (sqrt_one_minus_alpha_bar_t_m_1.square()) / (sqrt_one_minus_alpha_bar_t.square()) * (1 - alpha_t)
    y_t_m_1 = y_t_m_1_hat.to(device) + beta_t_hat.sqrt().to(device) * z.to(device)
    return y_t_m_1


# Reverse function -- sample y_0 given y_1
def p_sample_t_1to0(model, x, y, y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt):
    device = next(model.parameters()).device
    t = torch.tensor([0]).to(device)  # corresponding to timestep 1 (i.e., t=1 in diffusion models)
    sqrt_one_minus_alpha_bar_t = extract(one_minus_alphas_bar_sqrt, t, y)
    sqrt_alpha_bar_t = (1 - sqrt_one_minus_alpha_bar_t.square()).sqrt()
    eps_theta = model(x, y, y_0_hat, t).to(device).detach()
    # y_0 reparameterization
    y_0_reparam = 1 / sqrt_alpha_bar_t * (
            y - (1 - sqrt_alpha_bar_t) * y_T_mean - eps_theta * sqrt_one_minus_alpha_bar_t)
    y_t_m_1 = y_0_reparam.to(device)
    return y_t_m_1


def p_sample_loop(model, x, y_0_hat, y_T_mean, n_steps, alphas, one_minus_alphas_bar_sqrt):
    device = next(model.parameters()).device
    z = torch.randn_like(y_T_mean).to(device)
    cur_y = z + y_T_mean  # sample y_T
    y_p_seq = [cur_y]
    for t in reversed(range(1, n_steps)):  # t from T to 2
        y_t = cur_y
        cur_y = p_sample(model, x, y_t, y_0_hat, y_T_mean, t, alphas, one_minus_alphas_bar_sqrt)  # y_{t-1}
        y_p_seq.append(cur_y)
    assert len(y_p_seq) == n_steps
    y_0 = p_sample_t_1to0(model, x, y_p_seq[-1], y_0_hat, y_T_mean, one_minus_alphas_bar_sqrt)
    y_p_seq.append(y_0)
    return y_p_seq


# Evaluation with KLD
def kld(y1, y2, grid=(-20, 20), num_grid=400):
    y1, y2 = y1.numpy().flatten(), y2.numpy().flatten()
    p_y1, _ = np.histogram(y1, bins=num_grid, range=[grid[0], grid[1]], density=True)
    p_y1 += 1e-7
    p_y2, _ = np.histogram(y2, bins=num_grid, range=[grid[0], grid[1]], density=True)
    p_y2 += 1e-7
    return (p_y1 * np.log(p_y1 / p_y2)).sum()



================================================
FILE: regression/ema.py
================================================
import torch.nn as nn

class EMA(object):
    def __init__(self, mu=0.999):
        self.mu = mu
        self.shadow = {}

    def register(self, module):
        for name, param in module.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self, module):
        for name, param in module.named_parameters():
            if param.requires_grad:
                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data

    def ema(self, module):
        for name, param in module.named_parameters():
            if param.requires_grad:
                param.data.copy_(self.shadow[name].data)

    def ema_copy(self, module):
        module_copy = type(module)(module.config).to(module.config.device)
        module_copy.load_state_dict(module.state_dict())
        self.ema(module_copy)
        return module_copy

    def state_dict(self):
        return self.shadow

    def load_state_dict(self, state_dict):
        self.shadow = state_dict



================================================
FILE: regression/main.py
================================================
import argparse
import traceback
import shutil
import logging
import yaml
import sys
import os
import json
import torch
import numpy as np
import torch.utils.tensorboard as tb
import matplotlib.pyplot as plt
import time
from datetime import datetime, timezone, timedelta
from utils import *

torch.set_printoptions(sci_mode=False)

parser = argparse.ArgumentParser(description=globals()["__doc__"])

parser.add_argument(
    "--config", type=str, required=True, help="Path to the config file"
)
parser.add_argument('--device', type=int, default=0, help='GPU device id')
parser.add_argument('--thread', type=int, default=4, help='number of threads')
parser.add_argument("--seed", type=int, default=1234, help="Random seed")
parser.add_argument(
    "--exp", type=str, default="exp", help="Path for saving running related data."
)
parser.add_argument(
    "--doc",
    type=str,
    required=True,
    help="A string for documentation purpose. "
         "Will be the name of the log folder.",
)
parser.add_argument(
    "--comment", type=str, default="", help="A string for experiment comment"
)
parser.add_argument(
    "--verbose",
    type=str,
    default="info",
    help="Verbose level: info | debug | warning | critical",
)
parser.add_argument("--test", action="store_true", help="Whether to test the model")
parser.add_argument(
    "--sample",
    action="store_true",
    help="Whether to produce samples from the model",
)
parser.add_argument(
    "--train_guidance_only",
    action="store_true",
    help="Whether to only pre-train the guidance model f_phi",
)
parser.add_argument(
    "--run_all",
    action="store_true",
    help="Whether to run all train test splits",
)
parser.add_argument(
    "--noise_prior",
    action="store_true",
    help="Whether to apply a noise prior distribution at timestep T",
)
parser.add_argument(
    "--no_cat_f_phi",
    action="store_true",
    help="Whether to not concatenate f_phi as part of eps_theta input",
)
parser.add_argument("--fid", action="store_true")
parser.add_argument("--interpolation", action="store_true")
parser.add_argument(
    "--resume_training", action="store_true", help="Whether to resume training"
)
parser.add_argument(
    "-i",
    "--image_folder",
    type=str,
    default="images",
    help="The folder name of samples",
)
parser.add_argument(
    "--n_splits", type=int, default=20, help="total number of splits for a specific regression task"
)
parser.add_argument(
    "--split", type=int, default=0, help="which split to use for regression data"
)
parser.add_argument(
    "--init_split", type=int, default=0, help="initial split to train for regression data, usually for resume training"
)
parser.add_argument(
    "--rmse_timestep", type=int, default=0, help="selected timestep to report metric y RMSE"
)
parser.add_argument(
    "--qice_timestep", type=int, default=0, help="selected timestep to report metric y QICE"
)
parser.add_argument(
    "--picp_timestep", type=int, default=0, help="selected timestep to report metric y PICP"
)
parser.add_argument(
    "--nll_timestep", type=int, default=0, help="selected timestep to report metric y NLL"
)
parser.add_argument(
    "--ni",
    action="store_true",
    help="No interaction. Suitable for Slurm Job launcher",
)
parser.add_argument("--use_pretrained", action="store_true")
parser.add_argument(
    "--sample_type",
    type=str,
    default="generalized",
    help="sampling approach (generalized or ddpm_noisy)",
)
parser.add_argument(
    "--skip_type",
    type=str,
    default="uniform",
    help="skip according to (uniform or quadratic)",
)
parser.add_argument(
    "--timesteps", type=int, default=None, help="number of steps involved"
)
parser.add_argument(
    "--eta",
    type=float,
    default=0.0,
    help="eta used to control the variances of sigma",
)
parser.add_argument("--sequence", action="store_true")
# loss option
parser.add_argument(
    "--loss", type=str, default='ddpm', help="Which loss to use"
)
parser.add_argument("--nll_global_var", action="store_true",
                    help="Apply global variance for NLL computation")
parser.add_argument("--nll_test_var", action="store_true",
                    help="Apply sample variance of the test set for NLL computation")
# Conditional transport options
parser.add_argument(
    "--use_d",
    action="store_true",
    help="Whether to take an adversarially trained feature encoder",
)
parser.add_argument(
    "--full_joint",
    action="store_true",
    help="Whether to take fully joint matching",
)
parser.add_argument(
    "--num_sample", type=int, default=1, help="number of samples used in forward and reverse"
)
args = parser.parse_args()


def parse_config():
    # set log path
    args.log_path = os.path.join(args.exp, "logs", args.doc)
    # parse config file
    with open(os.path.join(args.config), "r") as f:
        if args.sample or args.test:
            config = yaml.unsafe_load(f)
            new_config = config
        else:
            config = yaml.safe_load(f)
            new_config = dict2namespace(config)

    tb_path = os.path.join(args.exp, "tensorboard", args.doc)

    if not args.test and not args.sample:
        args.im_path = os.path.join(args.exp, new_config.training.image_folder, args.doc)
        new_config.diffusion.noise_prior = True if args.noise_prior else False
        new_config.model.cat_y_pred = False if args.no_cat_f_phi else True
        if not args.resume_training:
            if not args.timesteps is None:
                new_config.diffusion.timesteps = args.timesteps
            if args.num_sample > 1:
                new_config.diffusion.num_sample = args.num_sample
            if os.path.exists(args.log_path):
                overwrite = False
                if args.ni:
                    overwrite = True
                else:
                    response = input("Folder {} already exists. Overwrite? (Y/N)".format(args.log_path))
                    if response.upper() == "Y":
                        overwrite = True

                if overwrite:
                    shutil.rmtree(args.log_path)
                    shutil.rmtree(tb_path)
                    shutil.rmtree(args.im_path)
                    os.makedirs(args.log_path)
                    os.makedirs(args.im_path)
                    if os.path.exists(tb_path):
                        shutil.rmtree(tb_path)
                else:
                    print("Folder exists. Program halted.")
                    sys.exit(0)
            else:
                os.makedirs(args.log_path)
                if not os.path.exists(args.im_path):
                    os.makedirs(args.im_path)

            with open(os.path.join(args.log_path, "config.yml"), "w") as f:
                yaml.dump(new_config, f, default_flow_style=False)

        new_config.tb_logger = tb.SummaryWriter(log_dir=tb_path)
        # setup logger
        level = getattr(logging, args.verbose.upper(), None)
        if not isinstance(level, int):
            raise ValueError("level {} not supported".format(args.verbose))

        handler1 = logging.StreamHandler()
        handler2 = logging.FileHandler(os.path.join(args.log_path, "stdout.txt"))
        formatter = logging.Formatter(
            "%(levelname)s - %(filename)s - %(asctime)s - %(message)s"
        )
        handler1.setFormatter(formatter)
        handler2.setFormatter(formatter)
        logger = logging.getLogger()
        logger.addHandler(handler1)
        logger.addHandler(handler2)
        logger.setLevel(level)

    else:
        if args.sample:
            args.im_path = os.path.join(args.exp, new_config.sampling.image_folder, args.doc)
        else:
            args.im_path = os.path.join(args.exp, new_config.testing.image_folder, args.doc)
        level = getattr(logging, args.verbose.upper(), None)
        if not isinstance(level, int):
            raise ValueError("level {} not supported".format(args.verbose))

        handler1 = logging.StreamHandler()
        # saving test metrics to a .txt file
        handler2 = logging.FileHandler(os.path.join(args.log_path, "testmetrics.txt"))
        formatter = logging.Formatter(
            "%(levelname)s - %(filename)s - %(asctime)s - %(message)s"
        )
        handler1.setFormatter(formatter)
        handler2.setFormatter(formatter)
        logger = logging.getLogger()
        logger.addHandler(handler1)
        logger.addHandler(handler2)
        logger.setLevel(level)

        if args.sample or args.test:
            os.makedirs(args.im_path, exist_ok=True)

    # add device
    device_name = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device_name)
    logging.info("Using device: {}".format(device))
    new_config.device = device

    # set number of threads
    if args.thread > 0:
        torch.set_num_threads(args.thread)
        print('Using {} threads'.format(args.thread))

    # set random seed
    if args.run_all:
        if new_config.data.dataset != "uci":
            args.seed += 1  # apply a different seed for each run of toy example
    set_random_seed(args.seed)

    torch.backends.cudnn.benchmark = True

    return new_config, logger


def main():
    config, logger = parse_config()

    logging.info("Writing log file to {}".format(args.log_path))
    logging.info("Exp instance id = {}".format(os.getpid()))
    logging.info("Exp comment = {}".format(args.comment))

    if args.loss == 'card_conditional':
        from card_regression import Diffusion
    else:
        raise NotImplementedError("Invalid loss option")

    try:
        runner = Diffusion(args, config, device=config.device)
        start_time = time.time()
        procedure = None
        if args.sample:
            runner.sample()
            procedure = "Sampling"
        elif args.test:
            y_rmse_all_steps_list, y_qice_all_steps_list, y_picp_all_steps_list, y_nll_all_steps_list = runner.test()
            procedure = "Testing"
        else:
            runner.train()
            procedure = "Training"
        end_time = time.time()
        logging.info("\n{} procedure finished. It took {:.4f} minutes.\n\n\n".format(
            procedure, (end_time - start_time) / 60))
        # remove logging handlers
        handlers = logger.handlers[:]
        for handler in handlers:
            logger.removeHandler(handler)
            handler.close()
        # return test metric lists
        if args.test:
            return y_rmse_all_steps_list, y_qice_all_steps_list, y_picp_all_steps_list, y_nll_all_steps_list, config
    except Exception:
        logging.error(traceback.format_exc())


if __name__ == "__main__":
    if args.run_all:
        y_rmse_all_splits_all_steps_list, y_qice_all_splits_all_steps_list, \
        y_picp_all_splits_all_steps_list, y_nll_all_splits_all_steps_list = [], [], [], []
        original_doc = args.doc
        original_config = args.config
        for split in range(args.init_split, args.n_splits):
            args.split = split
            # change "_split_" to "/split_" for future training
            # args.doc = original_doc + "_split_" + str(args.split)
            args.doc = original_doc + "/split_" + str(args.split)
            if args.test:
                args.config = original_config + args.doc + "/config.yml"
                y_rmse_all_steps_list, y_qice_all_steps_list, \
                y_picp_all_steps_list, y_nll_all_steps_list, config = main()
                y_rmse_all_splits_all_steps_list.append(y_rmse_all_steps_list)
                y_qice_all_splits_all_steps_list.append(y_qice_all_steps_list)
                y_picp_all_splits_all_steps_list.append(y_picp_all_steps_list)
                y_nll_all_splits_all_steps_list.append(y_nll_all_steps_list)
            else:
                main()

        # summary statistics across all splits
        if args.run_all and args.test:
            n_timesteps = config.diffusion.timesteps
            rmse_idx = n_timesteps - args.rmse_timestep
            qice_idx = n_timesteps - args.qice_timestep
            picp_idx = n_timesteps - args.picp_timestep
            nll_idx = n_timesteps - args.nll_timestep
            y_rmse_all_splits_list = [metric_list[rmse_idx] for metric_list in y_rmse_all_splits_all_steps_list]
            y_qice_all_splits_list = [metric_list[qice_idx] for metric_list in y_qice_all_splits_all_steps_list]
            y_picp_all_splits_list = [metric_list[picp_idx] for metric_list in y_picp_all_splits_all_steps_list]
            y_nll_all_splits_list = [metric_list[nll_idx] for metric_list in y_nll_all_splits_all_steps_list]

            print("\n\n================ Results Across Splits ================")
            print(f"y_RMSE mean: {np.mean(y_rmse_all_splits_list)} y_RMSE std: {np.std(y_rmse_all_splits_list)}")
            print(f"QICE mean: {np.mean(y_qice_all_splits_list)} QICE std: {np.std(y_qice_all_splits_list)}")
            print(f"PICP mean: {np.mean(y_picp_all_splits_list)} PICP std: {np.std(y_picp_all_splits_list)}")
            print(f"NLL mean: {np.mean(y_nll_all_splits_list)} NLL std: {np.std(y_nll_all_splits_list)}")

            # plot mean of all metric across all splits at all time steps during reverse diffusion
            y_rmse_all_splits_all_steps_array = np.array(y_rmse_all_splits_all_steps_list)
            y_qice_all_splits_all_steps_array = np.array(y_qice_all_splits_all_steps_list)
            y_picp_all_splits_all_steps_array = np.array(y_picp_all_splits_all_steps_list)
            y_nll_all_splits_all_steps_array = np.array(y_nll_all_splits_all_steps_list)
            y_rmse_mean_all_splits_list = [np.mean(
                y_rmse_all_splits_all_steps_array[:, idx]) for idx in range(n_timesteps + 1)]
            y_qice_mean_all_splits_list = [np.mean(
                y_qice_all_splits_all_steps_array[:, idx]) for idx in range(n_timesteps + 1)]
            y_picp_mean_all_splits_list = [np.mean(
                y_picp_all_splits_all_steps_array[:, idx]) for idx in range(n_timesteps + 1)]
            y_nll_mean_all_splits_list = [np.mean(
                y_nll_all_splits_all_steps_array[:, idx]) for idx in range(n_timesteps + 1)]
            n_metric = 4
            fig, axs = plt.subplots(n_metric, 1, figsize=(8.5, n_metric * 3))  # W x H
            plt.subplots_adjust(hspace=0.5)
            xticks = np.arange(0, n_timesteps + 1, config.diffusion.vis_step)
            # RMSE
            axs[0].plot(y_rmse_mean_all_splits_list)
            # axs[0].set_title('mean y RMSE of All Splits across All Timesteps', fontsize=14)
            axs[0].set_xlabel('timestep', fontsize=12)
            axs[0].set_xticks(xticks)
            axs[0].set_xticklabels(xticks[::-1])
            axs[0].set_ylabel('y RMSE', fontsize=12)
            # NLL
            axs[1].plot(y_nll_mean_all_splits_list)
            # axs[3].set_title('mean y NLL of All Splits across All Timesteps', fontsize=14)
            axs[1].set_xlabel('timestep', fontsize=12)
            axs[1].set_xticks(xticks)
            axs[1].set_xticklabels(xticks[::-1])
            axs[1].set_ylabel('y NLL', fontsize=12)
            # QICE
            axs[2].plot(y_qice_mean_all_splits_list)
            # axs[1].set_title('mean y QICE of All Splits across All Timesteps', fontsize=14)
            axs[2].set_xlabel('timestep', fontsize=12)
            axs[2].set_xticks(xticks)
            axs[2].set_xticklabels(xticks[::-1])
            axs[2].set_ylabel('y QICE', fontsize=12)
            # PICP
            picp_ideal = (config.testing.PICP_range[1]-config.testing.PICP_range[0])/100
            axs[3].plot(y_picp_mean_all_splits_list)
            axs[3].axhline(y=picp_ideal, c='b', label='95 % coverage')
            # axs[2].set_title('mean y PICP of All Splits across All Timesteps', fontsize=14)
            axs[3].set_xlabel('timestep', fontsize=12)
            axs[3].set_xticks(xticks)
            axs[3].set_xticklabels(xticks[::-1])
            axs[3].set_ylabel('y PICP', fontsize=12)
            axs[3].legend()

            # fig.suptitle('Mean y Metrics of All Splits across All Timesteps')
            im_path = os.path.join(args.exp, config.testing.image_folder, original_doc)
            if not os.path.exists(im_path):
                os.makedirs(im_path)
            fig.savefig(os.path.join(im_path, 'mean_metrics_all_splits_all_timesteps.pdf'))

            timestr = datetime.now(timezone(timedelta(hours=-6))).strftime("%Y%m%d-%H%M%S-%f")  # US Central time
            res_file_path = os.path.join(args.exp, "logs", original_doc, "metrics_all_splits")
            res_dict = {'n_splits': args.n_splits, 'task': original_doc,
                        'y_RMSE mean': float(np.mean(y_rmse_all_splits_list)),
                        'y_RMSE std': float(np.std(y_rmse_all_splits_list)),
                        'QICE mean': float(np.mean(y_qice_all_splits_list)),
                        'QICE std': float(np.std(y_qice_all_splits_list)),
                        'PICP mean': float(np.mean(y_picp_all_splits_list)),
                        'PICP std': float(np.std(y_picp_all_splits_list)),
                        'NLL mean': float(np.mean(y_nll_all_splits_list)),
                        'NLL std': float(np.std(y_nll_all_splits_list))
                        }
            args_dict = {'task': config.data.dataset,
                         'loss': args.loss,
                         'guidance': config.diffusion.conditioning_signal,
                         'n_timesteps': n_timesteps,
                         'n_splits': args.n_splits
                         }
            # save metrics and model hyperparameters to a json file
            if not os.path.exists(res_file_path):
                os.makedirs(res_file_path)
            with open(res_file_path + f"/metrics_{timestr}.json", "w") as outfile:
                json.dump(res_dict, outfile)
                outfile.write('\n\nExperiment arguments:\n')
                json.dump(args_dict, outfile)
            print("\nTest metrics saved in .json file.")
    else:
        args.doc = args.doc + "/split_" + str(args.split)
        if args.test:
            args.config = args.config + args.doc + "/config.yml"
        sys.exit(main())



================================================
FILE: regression/model.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F


class ConditionalLinear(nn.Module):
    def __init__(self, num_in, num_out, n_steps):
        super(ConditionalLinear, self).__init__()
        self.num_out = num_out
        self.lin = nn.Linear(num_in, num_out)
        self.embed = nn.Embedding(n_steps, num_out)
        self.embed.weight.data.uniform_()

    def forward(self, x, t):
        out = self.lin(x)
        gamma = self.embed(t)
        out = gamma.view(-1, self.num_out) * out
        return out


class ConditionalGuidedModel(nn.Module):
    def __init__(self, config):
        super(ConditionalGuidedModel, self).__init__()
        n_steps = config.diffusion.timesteps + 1
        self.cat_x = config.model.cat_x
        self.cat_y_pred = config.model.cat_y_pred
        data_dim = config.model.y_dim
        if self.cat_x:
            data_dim += config.model.x_dim
        if self.cat_y_pred:
            data_dim += config.model.y_dim
        self.lin1 = ConditionalLinear(data_dim, 128, n_steps)
        self.lin2 = ConditionalLinear(128, 128, n_steps)
        self.lin3 = ConditionalLinear(128, 128, n_steps)
        self.lin4 = nn.Linear(128, 1)

    def forward(self, x, y_t, y_0_hat, t):
        if self.cat_x:
            if self.cat_y_pred:
                eps_pred = torch.cat((y_t, y_0_hat, x), dim=1)
            else:
                eps_pred = torch.cat((y_t, x), dim=1)
        else:
            if self.cat_y_pred:
                eps_pred = torch.cat((y_t, y_0_hat), dim=1)
            else:
                eps_pred = y_t
        eps_pred = F.softplus(self.lin1(eps_pred, t))
        eps_pred = F.softplus(self.lin2(eps_pred, t))
        eps_pred = F.softplus(self.lin3(eps_pred, t))
        return self.lin4(eps_pred)


# deterministic feed forward neural network
class DeterministicFeedForwardNeuralNetwork(nn.Module):

    def __init__(self, dim_in, dim_out, hid_layers,
                 use_batchnorm=False, negative_slope=0.01, dropout_rate=0):
        super(DeterministicFeedForwardNeuralNetwork, self).__init__()
        self.dim_in = dim_in  # dimension of nn input
        self.dim_out = dim_out  # dimension of nn output
        self.hid_layers = hid_layers  # nn hidden layer architecture
        self.nn_layers = [self.dim_in] + self.hid_layers  # nn hidden layer architecture, except output layer
        self.use_batchnorm = use_batchnorm  # whether apply batch norm
        self.negative_slope = negative_slope  # negative slope for LeakyReLU
        self.dropout_rate = dropout_rate
        layers = self.create_nn_layers()
        self.network = nn.Sequential(*layers)

    def create_nn_layers(self):
        layers = []
        for idx in range(len(self.nn_layers) - 1):
            layers.append(nn.Linear(self.nn_layers[idx], self.nn_layers[idx + 1]))
            if self.use_batchnorm:
                layers.append(nn.BatchNorm1d(self.nn_layers[idx + 1]))
            layers.append(nn.LeakyReLU(negative_slope=self.negative_slope))
            layers.append(nn.Dropout(p=self.dropout_rate))
        layers.append(nn.Linear(self.nn_layers[-1], self.dim_out))
        return layers

    def forward(self, x):
        return self.network(x)


# early stopping scheme for hyperparameter tuning
class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""

    def __init__(self, patience=10, delta=0):
        """
        Args:
            patience (int): Number of steps to wait after average improvement is below certain threshold.
                            Default: 10
            delta (float): Minimum change in the monitored quantity to qualify as an improvement;
                           shall be a small positive value.
                           Default: 0
            best_score: value of the best metric on the validation set.
            best_epoch: epoch with the best metric on the validation set.
        """
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.best_epoch = None
        self.early_stop = False

    def __call__(self, val_cost, epoch, verbose=False):

        score = val_cost

        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch + 1
        elif score > self.best_score - self.delta:
            self.counter += 1
            if verbose:
                print("EarlyStopping counter: {} out of {}...".format(
                    self.counter, self.patience))
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_epoch = epoch + 1
            self.counter = 0


================================================
FILE: regression/utils.py
================================================
import random
import logging
import numpy as np
import argparse
import torch
import torch.optim as optim
from torch import nn
from data_loader import *


def set_random_seed(seed):
    print(f"\n* Set seed {seed}")
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)


def dict2namespace(config):
    namespace = argparse.Namespace()
    for key, value in config.items():
        if isinstance(value, dict):
            new_value = dict2namespace(value)
        else:
            new_value = value
        setattr(namespace, key, new_value)
    return namespace


def multiple_experiments(
        model_class,
        data_class,
        OLS_class,
        data_init_dict: dict,
        train_test_dict: dict,
        ols_init_dict: dict,
        ols_fit_dict: dict,
        model_init_dict: dict,
        model_train_dict: dict,
        model_eval_dict: dict,
        seeds=None,
        num_seeds=10,
        model_to_plot=None,
        ols_plot_dict=None,
        device="cpu"
):
    if seeds is None:
        seeds = np.arange(num_seeds)
    if model_to_plot is None:
        model_to_plot = seeds[-1]
    ols_coverages = []
    coverages = []

    for idx, seed in enumerate(seeds):
        data_init_dict.update(seed=seed)
        data = data_class(**data_init_dict)
        data.create_train_test_dataset(**train_test_dict)

        ols_init_dict.update(data=data)
        ols_model = OLS_class(**ols_init_dict)
        ols_model.fit_model(**ols_fit_dict)
        ols_model.get_test_prediction_coverage()
        ols_coverages.append(ols_model.test_prediction_coverage)
        if ols_plot_dict is not None and idx == model_to_plot:
            ols_model.plot(**ols_plot_dict)

        model_init_dict.update(seed=seed, dim_x=data.dim_x, dim_y=data.dim_y)
        model = model_class(**model_init_dict).to(device)
        print(f"\n*Create the {idx}-th model from class {type(model).__name__}, with random seed {model.seed}")

        model_train_dict.update(dataset=data)
        model.train_loop(**model_train_dict)

        model_eval_dict.update(dataset=data, make_plot=False)
        coverages.append(model.evaluate(**model_eval_dict))

        if idx == model_to_plot:
            model_eval_dict.update({"make_plot": True})
            for plot_true in (True, False):
                for plot_gen in (True, False):
                    model_eval_dict.update(plot_true=plot_true, plot_gen=plot_gen)
                    model.evaluate(**model_eval_dict)

        print(f"\n* For class {type(model).__name__} and {len(seeds)} seeds: mean coverage rate: {np.mean(coverages):.4f}, std: {np.std(coverages):.4f}")
        print(f"\n* For class {type(ols_model).__name__} and {len(seeds)} seeds: mean coverage rate: {np.mean(ols_coverages):.4f}, std: {np.std(ols_coverages):.4f}")
        return coverages, ols_coverages


# TODO: Add evaluation metrics


class SubspaceInferenceDatasetNet(nn.Sequential):
    def __init__(self, dimensions=(200, 50, 50, 50), input_dim=2, output_dim=1):
        super(SubspaceInferenceDatasetNet, self).__init__()
        self.dimensions = [input_dim, *dimensions, output_dim]
        for i in range(len(self.dimensions) - 1):
            self.add_module('linear%d' % i, torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1]))
            if i < len(self.dimensions) - 2:
                self.add_module('relu%d' % i, torch.nn.ReLU())

    def forward(self, x, output_features=False):
        if not output_features:
            return super().forward(x)
        else:
            print(self._modules.values())
            print(list(self._modules.values())[:-2])
            for module in list(self._modules.values())[:-3]:
                x = module(x)
                print(x.size())
            return x
        

def sizeof_fmt(num, suffix='B'):
    """
    https://stackoverflow.com/questions/24455615/python-how-to-display-size-of-all-variables
    """
    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:
        if abs(num) < 1024.0:
            return "%3.1f %s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f %s%s" % (num, 'Yi', suffix)


# print("Check memory usage of different variables:")
# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),
#                          key=lambda x: -x[1])[:10]:
#     print("{:>30}: {:>8}".format(name, sizeof_fmt(size)))


def get_optimizer(config_optim, parameters):
    if config_optim.optimizer == 'Adam':
        return optim.Adam(parameters, lr=config_optim.lr, weight_decay=config_optim.weight_decay,
                          betas=(config_optim.beta1, 0.999), amsgrad=config_optim.amsgrad,
                          eps=config_optim.eps)
    elif config_optim.optimizer == 'RMSProp':
        return optim.RMSprop(parameters, lr=config_optim.lr, weight_decay=config_optim.weight_decay)
    elif config_optim.optimizer == 'SGD':
        return optim.SGD(parameters, lr=config_optim.lr, momentum=0.9)
    else:
        raise NotImplementedError(
            'Optimizer {} not understood.'.format(config_optim.optimizer))
    
    
def get_optimizer_and_scheduler(config, parameters, epochs, init_epoch):
    scheduler = None
    optimizer = get_optimizer(config, parameters)
    if hasattr(config, "T_0"):
        T_0 = config.T_0
    else:
        T_0 = epochs // (config.n_restarts + 1)
    if config.use_scheduler:
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,
                                                                   T_0=T_0,
                                                                   T_mult=config.T_mult,
                                                                   eta_min=config.eta_min,
                                                                   last_epoch=-1)
        scheduler.last_epoch = init_epoch - 1
    return optimizer, scheduler


def get_dataset(args, config, test_set=False, validation=False):
    data_object = None
    if config.data.dataset == 'swiss_roll':
        data = SwissRoll().sample(config.data.dataset_size)
    elif config.data.dataset == 'moons':
        data = Moons().sample(config.data.dataset_size)
    elif config.data.dataset == '8gaussians':
        data = Gaussians().sample(config.data.dataset_size - config.data.dataset_size % 8, mode=8)
    elif config.data.dataset == '25gaussians':
        data = Gaussians().sample(config.data.dataset_size - config.data.dataset_size % 25, mode=25)
    elif config.data.dataset == "uci":
        data_object = UCI_Dataset(config, args.split, validation)
        data_type = "test" if test_set else "train"
        logging.info(data_object.summary_dataset(split=data_type))
        data = data_object.return_dataset(split=data_type)
    elif config.data.dataset == "linear_regression":
        data_object = LinearDatasetWithOneX(a=config.data.a, b=config.data.b,
                                            n_samples=config.data.dataset_size,
                                            seed=args.seed,
                                            x_dict=vars(config.data.x_dict),
                                            noise_dict=vars(config.data.noise_dict),
                                            normalize_x=config.data.normalize_x,
                                            normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "quadratic_regression":
        data_object = QuadraticDatasetWithOneX(a=config.data.a, b=config.data.b, c=config.data.c,
                                               n_samples=config.data.dataset_size,
                                               seed=args.seed,
                                               x_dict=vars(config.data.x_dict),
                                               noise_dict=vars(config.data.noise_dict),
                                               normalize_x=config.data.normalize_x,
                                               normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "sinusoidal_regression_mdn":
        data_object = SinusoidDatasetWithOneX(n_samples=config.data.dataset_size,
                                              seed=args.seed,
                                              x_dict=vars(config.data.x_dict),
                                              noise_dict=vars(config.data.noise_dict),
                                              normalize_x=config.data.normalize_x,
                                              normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "inverse_sinusoidal_regression_mdn":
        data_object = SinusoidDatasetWithOneX(n_samples=config.data.dataset_size,
                                              seed=args.seed,
                                              x_dict=vars(config.data.x_dict),
                                              noise_dict=vars(config.data.noise_dict),
                                              normalize_x=config.data.normalize_x,
                                              normalize_y=config.data.normalize_y)
        data_object.invert_xy()
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "full_circle":
        data_object = CircleDatasetWithOneX(r=config.data.r, n_samples=config.data.dataset_size,
                                            seed=args.seed,
                                            x_dict=vars(config.data.x_dict),
                                            noise_dict=vars(config.data.noise_dict),
                                            normalize_x=config.data.normalize_x,
                                            normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "loglog_linear_regression":
        data_object = LogLogDatasetWithOneX(a=config.data.a, b=config.data.b,
                                            n_samples=config.data.dataset_size,
                                            seed=args.seed,
                                            x_dict=vars(config.data.x_dict),
                                            noise_dict=vars(config.data.noise_dict),
                                            normalize_x=config.data.normalize_x,
                                            normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "loglog_cubic_regression":
        data_object = LogLogDatasetWithOneX(a=config.data.a, b=config.data.b,
                                            n_samples=config.data.dataset_size,
                                            seed=args.seed,
                                            x_dict=vars(config.data.x_dict),
                                            noise_dict=vars(config.data.noise_dict),
                                            normalize_x=config.data.normalize_x,
                                            normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    elif config.data.dataset == "8gauss":
        data_object = CTToyDataset(n_samples=config.data.dataset_size,
                                   seed=args.seed,
                                   x_dict=vars(config.data.x_dict),
                                   noise_dict=vars(config.data.noise_dict),
                                   normalize_x=config.data.normalize_x,
                                   normalize_y=config.data.normalize_y)
        data_object.create_train_test_dataset(train_ratio=config.data.train_ratio)
        data = data_object.test_dataset if test_set else data_object.train_dataset
    else:
        raise NotImplementedError(
            "Toy dataset options: swiss_roll, moons, 8gaussians and 25gaussians; regression data: UCI.")
    return data_object, data




================================================
FILE: regression/configs/toy_8gauss.yml
================================================
data:
    dataset: "8gauss"
    true_function: None
    dataset_size: 10240
    seed: 2000
    no_multimodality: False
    inverse_xy: False
    x_dict: {"ct_toy_name": "8gaussians"}
    noise_dict: {}
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: True
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/toy_full_circle.yml
================================================
data:
    dataset: "full_circle"
    true_function: None
    dataset_size: 10240
    seed: 2000
    no_multimodality: False
    inverse_xy: False
    x_dict: {}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 0.5}
    r: 10
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: True
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0



================================================
FILE: regression/configs/toy_inverse_sinusoidal_regression_mdn.yml
================================================
data:
    dataset: "inverse_sinusoidal_regression_mdn"
    true_function: "mdnsinusoidal"
    dataset_size: 10240
    seed: 2000
    no_multimodality: False
    inverse_xy: True
    x_dict: {"dist_type": "unif", "low": 0, "high": 1}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 0.08}
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.4
    timesteps: 50
    vis_step: 5
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 500
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0



================================================
FILE: regression/configs/toy_linear_regression.yml
================================================
data:
    dataset: "linear_regression"
    true_function: "linear"
    dataset_size: 10240
    seed: 2000
    no_multimodality: True
    inverse_xy: False
    x_dict: {"dist_type": "unif", "low": -5, "high": 5}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 2}
    a: 2
    b: 3
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [10]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0



================================================
FILE: regression/configs/toy_loglog_cubic_regression.yml
================================================
data:
    dataset: "loglog_cubic_regression"
    true_function: "loglog"
    dataset_size: 10240
    seed: 2000
    no_multimodality: True
    inverse_xy: False
    x_dict: {"dist_type": "unif", "low": 0, "high": 10}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 0.15}
    a: 1
    b: 3
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: True

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.4
    timesteps: 50
    vis_step: 5
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 500
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0



================================================
FILE: regression/configs/toy_loglog_linear_regression.yml
================================================
data:
    dataset: "loglog_linear_regression"
    true_function: "loglog"
    dataset_size: 10240
    seed: 2000
    no_multimodality: True
    inverse_xy: False
    x_dict: {"dist_type": "unif", "low": 0, "high": 10}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 0.15}
    a: 1
    b: 1
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0



================================================
FILE: regression/configs/toy_quadratic_regression.yml
================================================
data:
    dataset: "quadratic_regression"
    true_function: "quadratic"
    dataset_size: 10240
    seed: 2000
    no_multimodality: True
    inverse_xy: False
    x_dict: {"dist_type": "unif", "low": -5, "high": 5}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 2}
    a: 3
    b: 2
    c: 1
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal, cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [50, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 16000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0



================================================
FILE: regression/configs/toy_sinusoidal_regression_mdn.yml
================================================
data:
    dataset: "sinusoidal_regression_mdn"
    true_function: "mdnsinusoidal"
    dataset_size: 10240
    seed: 2000
    no_multimodality: True
    inverse_xy: False
    x_dict: {"dist_type": "unif", "low": 0, "high": 1}
    noise_dict: {"noise_type": "norm", "loc": 0, "scale": 0.08}
    train_ratio: 0.8
    num_workers: 0
    normalize_x: False
    normalize_y: False

model:
    type: "simple"
    data_dim: 2
    x_dim: 1
    y_dim: 1
    z_dim: 2
    cat_x: True
    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine_anneal
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: False
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 1000
    validation_freq: 128000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 256
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    vis_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: True
    squared_plot: False
    plot_true: True
    plot_gen: True
    fig_size: [8, 5]
    one_fig_size: [4, 4]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_boston.yml
================================================
data:
    dataset: "uci"
    dir: "bostonHousing"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: True
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 14
    x_dim: 13
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 300
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.2
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 32
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_concrete.yml
================================================
data:
    dataset: "uci"
    dir: "concrete"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 9
    x_dim: 8
    y_dim: 1
    z_dim: 2
    cat_x: True
#    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear  # cosine
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 200
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.1
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 32
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_energy.yml
================================================
data:
    dataset: "uci"
    dir: "energy"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: True
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 15
    x_dim: 14
    # data_dim: 9
    # x_dim: 8
    y_dim: 1
    z_dim: 2
    cat_x: True
#    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.05
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 32
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_kin8nm.yml
================================================
data:
    dataset: "uci"
    dir: "kin8nm"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 9
    x_dim: 8
    y_dim: 1
    z_dim: 2
    cat_x: True
#    cat_y_pred: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 300
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.1
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 64
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 10
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_naval.yml
================================================
data:
    dataset: "uci"
    dir: "naval-propulsion-plant"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: True
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 29
    x_dim: 28
    # data_dim: 17
    # x_dim: 16
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.0
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 64
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 10
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_power.yml
================================================
data:
    dataset: "uci"
    dir: "power-plant"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 5
    x_dim: 4
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 200
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.1
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 64
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 128
    sampling_size: 1000
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_protein.yml
================================================
data:
    dataset: "uci"
    dir: "protein-tertiary-structure"
    n_splits: 5
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 10
    x_dim: 9
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 500
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.1
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 10
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [ 8, 5 ]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_wine.yml
================================================
data:
    dataset: "uci"
    dir: "wine-quality-red"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 12
    x_dim: 11
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 200
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.2
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 32
    n_epochs: 5000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 2000
    validation_freq: 20000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 5
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_yacht.yml
================================================
data:
    dataset: "uci"
    dir: "yacht"
    n_splits: 20
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 7
    x_dim: 6
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.2
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 32
    n_epochs: 10000
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 5000
    validation_freq: 50000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 2
    image_folder: 'testing_image_samples'
    n_z_samples: 1000
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0
    
aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/configs/uci_year.yml
================================================
data:
    dataset: "uci"
    dir: "YearPredictionMSD"
    n_splits: 1
    num_workers: 0
    one_hot_encoding: False
    normalize_x: True
    normalize_y: True
    data_root: "/data/datasets/UCI_Datasets"

model:
    type: "simple"
    data_dim: 91
    x_dim: 90
    y_dim: 1
    z_dim: 2
    cat_x: True
    feature_dim: 128
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    timesteps: 1000
    vis_step: 100
    num_figs: 10
    conditioning_signal: "NN"
    nonlinear_guidance:
        pre_train: True
        joint_train: False
        n_pretrain_epochs: 100
        logging_interval: 10
        hid_layers: [100, 50]
        use_batchnorm: False
        negative_slope: 0.01
        dropout_rate: 0.2
        apply_early_stopping: True
        n_pretrain_max_epochs: 1000
        train_ratio: 0.6  # for splitting original train into train and validation set for hyperparameter tuning
        patience: 50
        delta: 0  # hyperparameter for improvement measurement in the early stopping scheme

training:
    batch_size: 256
    n_epochs: 500
    n_iters: 100000
    snapshot_freq: 1000000000
    logging_freq: 5000
    validation_freq: 50000
    image_folder: 'training_image_samples'

sampling:
    batch_size: 32
    sampling_size: 1000
    last_only: True
    image_folder: 'sampling_image_samples'

testing:
    batch_size: 64
    sampling_size: 1000
    last_only: True
    plot_freq: 200
    image_folder: 'testing_image_samples'
    n_z_samples: 200
    n_bins: 10
    compute_metric_all_steps: True
    mean_t: 0
    coverage_t: 0
    nll_t: 0
    trimmed_mean_range: [0.0, 100.0]
    PICP_range: [2.5, 97.5]
    make_plot: False
    squared_plot: False
    plot_true: False
    plot_gen: False
    fig_size: [8, 5]

optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0

aux_optim:
    weight_decay: 0.000
    optimizer: "Adam"
    lr: 0.001
    beta1: 0.9
    amsgrad: True
    eps: 0.00000001
    grad_clip: 1.0


================================================
FILE: regression/training_scripts/run_toy_8gauss.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_8gauss
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_full_circle.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_full_circle
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_inverse_sinusoidal_regression_mdn.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_inverse_sinusoidal_regression_mdn
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_linear_regression.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_linear_regression
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_loglog_cubic_regression.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_loglog_cubic_regression
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_loglog_linear_regression.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_loglog_linear_regression
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_quadratic_regression.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_quadratic_regression
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_toy_sinusoidal_regression_mdn.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=toy_sinusoidal_regression_mdn
export N_SPLITS=10
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_toy_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_boston.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_boston
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=1

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_concrete.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_concrete
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=0

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_energy.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_energy
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_kin8nm.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_kin8nm
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=1

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_naval.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_2
export LOSS=card_conditional
export TASK=uci_naval
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=3

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_power.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_power
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=2

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_protein.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_protein
export N_SPLITS=5
export N_THREADS=4
export DEVICE_ID=0

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_wine.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_wine
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=0

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_yacht.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_yacht
export N_SPLITS=20
export N_THREADS=4
export DEVICE_ID=1

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test


================================================
FILE: regression/training_scripts/run_uci_year.sh
================================================
export EXP_DIR=./results
export N_STEPS=1000
export SERVER_NAME=a4000
export RUN_NAME=run_1
export LOSS=card_conditional
export TASK=uci_year
export N_SPLITS=1
export N_THREADS=4
export DEVICE_ID=1

export CAT_F_PHI=_cat_f_phi
export MODEL_VERSION_DIR=card_conditional_uci_results/${N_STEPS}steps/nn/${RUN_NAME}_${SERVER_NAME}/f_phi_prior${CAT_F_PHI}
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config configs/${TASK}.yml #--train_guidance_only
python main.py --device ${DEVICE_ID} --thread ${N_THREADS} --loss ${LOSS} --exp $EXP_DIR/${MODEL_VERSION_DIR} --run_all --n_splits ${N_SPLITS} --doc ${TASK} --config $EXP_DIR/${MODEL_VERSION_DIR}/logs/ --test

